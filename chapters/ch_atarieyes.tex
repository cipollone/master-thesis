\chapter{AtariEyes package}

\label{ch:atarieyes}

In this Chapter, we'll discuss the software realized in this thesis, called
``AtariEyes''. Its purpose is to implement the ideas that have been presented
until this point. Every experiment of Chapter~\ref{ch:experiments} has been
obtained with this software.

Apart from the important aspect of realizing the models we've defined, this
software has some interesting qualities:
\begin{description}
	\item [Clarity] Because it includes a complete documentation of methods
		and structures.
	\item [Efficiency] Thanks to an heavy use of parallel computing libraries
		for GPU acceleration.
	\item [User friendly] The extensive command line interface allows to
		experiment with the package as it is, or, thanks to its modular design,
		individual structures can be reused in future developments.
\end{description}

Regarding the general functionality of the package, through the commands
provided, the user can:
\begin{itemize}
	\item Choose any \emph{environment} from the Atari~2600 collection.
	\item Train a Deep Reinforcement Learning \emph{agent}. The algorithm is
		Double DQN and the agent's model can be either the original Atari model
		(Section~\ref{sec:model-atari}) or the restricted agent
		(Section~\ref{sec:model-atari-rb}).
	\item Train the \emph{feature extractor}. This implements the whole model
		presented in Chapter~\ref{ch:fluents}.
	\item \emph{Play}, \emph{visualize} and \emph{record} any of these
		agents while they interact with the environment.
\end{itemize}

This chapter contains two sections: Section~\ref{sec:how-to-use} documents
how the software can be used from a user perspective;
Section~\ref{sec:implementation} is a larger part that explains some of the
most interesting details about the implementation.


\section{How to use the software}

\label{sec:how-to-use}

% Most common are bash commands here
\lstset{style=bash}

\subsection{Tools and setup}

The software \texttt{AtariEyes} is a Python package. It is publicly available
at the GitHub repository:
\href{https://github.com/cipollone/atarieyes}{\texttt{cipollone/atarieyes}}.
As any other Python package, it can be installed with the \texttt{pip}
command; we just need to point to this git repository. The installation
command is:
\begin{lstlisting}
pip install git+https://github.com/cipollone/atarieyes
\end{lstlisting}
This installs this package from the master branch. If we need to work on some
specific revision, for example on the \texttt{develop} branch, we can append
\verb!@develop! or any other commit to the previous command.

Dependencies are automatically installed by \texttt{pip}.  In Python, it is
common to run applications inside virtual environments. Just run this
installation command from a container to avoid dependency conflicts with
other applications. One rather particular dependency is TensorFlow, which is a
famous Machine Learning library that allows parallel computing. Following the
instructions of the specific container application, we can link to some
preexisting system installation if we need it. Currently, the supported
version is only 2.1, but future versions might also be compatible.

The package is written in Python~3 and the minimum version required for the
interpreter is 3.7. This requirement should be met by most modern operating
systems. If that's not the case, we suggest to use \texttt{pyenv}, which
allows environment-specific Python installations.

This procedure installs the \texttt{atarieyes} package. As we will see, we
usually use this module through its command line interface. If we want to
include some structures in future developments, we can
\lstinline[style=inlinepy]|import atarieyes|, as usual. However, for
development, it might be useful to look at the source code, by cloning it:
\begin{lstlisting}
git clone https://github.com/cipollone/atarieyes.git
\end{lstlisting}
This is also useful if, for any reason, some updated dependency is no longer
compatible with this package. What we can do is to \texttt{cd} to this cloned
directory, then run \texttt{poetry install}. Poetry is the container
application application that we use. When we run this command, it will install
the exact dependency versions that have been used during development.


\subsection{Commands}

To run this package as a script we run this command from the same environment
where we've installed it:
\begin{lstlisting}
python3 -m atarieyes
\end{lstlisting}
To save space, the shell commands that we will see start with
\texttt{atarieyes}, but we would always run them through
\lstinline[style=inlinesh]|python3 -m|.

\subsubsection*{Getting help}

The package provides a compete command line interface with many options to
control the training process. This section is a reference and explanation of
many of these. For any doubt, we can use the \verb|--help| option, abbreviated
as \verb|-h|. This option prints the arguments that are supported for any
command. For example, running \lstinline[style=inlinesh]{atarieyes -h}
produces the following message\footnote{This output is generated by the
\texttt{argparse} library that we use.}:
\begin{lstlisting}[language={}]
usage: __main__.py [-h] [--list] [--from FROM] {agent,features} ...

Feature extraction and RL on Atari Games

positional arguments:
  {agent,features}  Choose group
    agent           Reinforcement Learning agent
    features        Features extraction

optional arguments:
  -h, --help        show this help message and exit
  --list            List all environments, then exit
  --from FROM       Load arguments from file
\end{lstlisting}

The \verb|--list| option prints the unique names of all the Atari games. Any
game from this list can be used as environment to train or experiment with
agents. To select an environment we pass any of these names to the
\verb|--env|/\texttt{-e} option, where appropriate.

The \verb|--from| option allows to run according to the commands and options
stored some JSON file. The JSON must be a dictionary of pairs: argument
name--argument value. The interface of this file is exactly the same of the
command line interface that we're describing. After any ``train'' command, an
\verb|args.json| is automatically saved. The purpose of this option is to
repeat, resume or slightly modify the same options that were previously used.

The main commands are then grouped in two groups. Those regarding the agent,
more precisely the RL agent, and those regarding the features extractor.


\subsubsection*{Agents}

There are three operations regarding the agent: \texttt{train},
\texttt{play}, and \texttt{watch}.

To train an agent we run:
\begin{lstlisting}
atarieyes agent train /*_ \dots _*/
\end{lstlisting}
This command has many options, some of which control the parameters of the
Double DQN algorithm. We show here just the most relevant:
\begin{description}
	\item [\texttt{-e}/\texttt{--env}] Selects the environment to use among the
		list of Atari games.
	\item [\texttt{-b}/\texttt{--batch}] Each update of the Q-Network is
		computed from a cumulative gradient of this number of samples.
	\item [\texttt{-r}/\texttt{--rate}] Chooses the learning rate associated
		to each gradient update.
	\item [\texttt{-g}/\texttt{--gamma}] Selects a discount factor.
	\item [\texttt{-c}/\texttt{--continue}] Resumes training from any
		checkpoint. Checkpoints are saved in regular intervals (according to the
		\texttt{-s} option) or when a training is interrupted with CTRL-C
		(SIGINT).
	\item [\texttt{--rb}] Trains an agent with the Restraining Bolt applied.
		When this option is absent, the agent Q-Network is that of
		Section~\ref{sec:model-atari}. When \texttt{--rb} is added the
		restrained model is used, from Section~\ref{sec:model-atari-rb}.
		The argument of this command is the IP of a running Restraining Bolt;
		often, just \texttt{localhost}.
\end{description}
There are many other options which we didn't list here.

The second command related to agents is \texttt{play}. Its purpose is to load
an agent previously trained and let it interact with the environment. This is
certainly useful to assess the performances reached. More importantly, this
continuous play can generate the stream of observations that we need when
training the features extractor. Some options are:
\begin{description}
	\item [\texttt{args\_file}] This mandatory argument is the path of the JSON
		file containing the exact training command that generated the agent.
	\item [\texttt{-c}/\texttt{--continue}] It is a mandatory argument that
		says which checkpoint to load.
	\item [\texttt{--rand-eps} {\normalfont and} \texttt{--explore-policy}]
		These two options allow to use the two custom exploration policies that
		were defined in Section~\ref{sec:exploration-policies}.
	\item [\texttt{-w}/\texttt{--watch}] To visualize the frames of the game.
		We can render the images on screen, or we can send them to another running
		instance. The most useful use is when sending the frames to train the
		features extractor model.
	\item [\texttt{--record}] To save a video of the agent's performance.
\end{description}

% TODO: training output
% TODO: rb -- agent interaction

\section{Implementation}

\label{sec:implementation}

\subsection{\texttt{agent} Module}

\label{sec:impl-agent}

\subsubsection{\texttt{training} Module}
\subsubsection{\texttt{playing} Module}

\subsection{\texttt{streaming} Module}
\subsection{\texttt{features} Module}
\subsubsection{\texttt{models} Module}
\subsubsection{\texttt{genetic} Module}
\subsubsection{\texttt{temporal} Module}

% TODO: I've contributed to flloat

