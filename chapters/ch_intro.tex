\chapter{Introduction}

% Intro logic
A classic and important branch of Artificial Intelligence (AI) aims at
developing agents that select their actions through a form of logic reasoning,
such as planning. One of the main advantages of these approaches is that
reasoning proceeds by manipulating \emph{abstractions}. In fact, in logic,
we can define symbols that represent any meaningful event or condition that
should be considered. For example, some propositional symbols might represent
conditions such as ``the door is closed'' or ``I am holding an object'', etc.
We'll also call these propositional symbols with the term ``\emph{fluents}''
(a name that suggests how their truth can change over time).
% TODO: read again

% Grounding
These reasoning methods based on logics are powerful but they imply one
fundamental ability: at each instant, the agent must be able to decide whether
those propositions are true. This means that all symbols that represent
conditions which happen to be true in the environment, must be true for the
agent. This ``grounding'' process can be really hard in complex environments,
because the agent's sensors may return a noisy and multidimensional output,
that is difficult to interpret.

% Observations in RL
Reinforcement Learning (RL) is a successful field of AI, in which the
agent's goal is to learn a policy that maximize the rewards received.  We
could argue that RL does not require the valuations just mentioned. Still,
rewards and punishments must be somehow supplied in response to desirable and
undesirable events. We could consider of providing these feedbacks with
programmed ad-hoc conditions, but this can be easily done just for the
simulations we create. Furthermore, as we will see, some complex tasks can
only be solved through a combination of both RL and logic-based methods; thus,
introducing all the needs of the latter.

% Bridge: goal
With this thesis, we defined and implemented an agent based on temporal logics
and Deep Reinforcement Learning. This required to investigate new ways to
solve the fluents valuation problem that has been just described, for a
specific class of environments and fluents. In
Section~\ref{sec:intro-objective}, the goal and the achievements of this work
will be described more precisely.


\section{Related works}

\label{sec:intro-related}

% RL
Reinforcement Learning (RL) is an area of Machine Learning in which the agent
is trained by sending rewards and punishments in response to its actions.
This technique can be also used unknown environments, where a model of the
dynamics is not available, because the agent learns by trying all actions and
by remembering those that lead to the highest rewards. As we will see in
Chapter~\ref{ch:rl}, most RL algorithms assume that the environment can be
modelled with a Markov Decision Process (MDP). Many learning algorithms exist
in this setting~\cite{bib:rl-book}.

% Deep RL
Neural Networks (NN) have brought new possibilities for RL: in Deep
Reinforcement Learning (Deep RL), the agent employs a neural network as a very
expressive function approximator for the quantities it is trying to
learn~\cite{bib:deep-rl}. For example, the optimal q-value is an important
quantity in RL, that the agents are usually designed to learn from the
observations received. The Deep Q-Network (DQN)
algorithm~\cite{bib:atari-deeprl} is one the first to successfully employ
neural networks in RL. They have shown that a Deep RL agent can be trained
directly from complex observations such as the frames of a video game. Without
modifications, the same agent has been able to reach human-level performances
in many games.

% Atari games
Games have always been a classic benchmark for AI algorithms, because they
provide various levels of complexity, they have few and strict rules, and they
are easy to implement and simulate. Regarding Deep RL, many authors have
tested their algorithms on the collection of video games
``Atari~2600''~\cite{bib:atari-games}. In this thesis, we'll use and
experiment with the same environments.

% Algorithms
The reinforcement learning algorithm we've adopted is called Double
DQN~\cite{bib:double-q}. The motivation of this choice is that this is a
relatively simple algorithm, based on DQN, which has also proven to be
successful for the specific environments that we'll use in our
experiments~\cite{bib:atari-deepq-nature}. In fact, among Q-Network
algorithms, the only ones that were able to clearly achieve superior
performances in most of these games combined a combination of all DQN
variants~\cite{bib:rainbow}.

% Some games are hard
If we look at the results in~\cite{bib:atari-deepq-nature}, DQN agents are
able to learn excellent policies for many games. However, for many other
environments of the same collection, the agents struggle to learn and, in some
cases, they don't learn anything at all. The worst performances have been
measured for the \emph{Montezuma's Revenge} environment. Even in the works
that followed, the only methods that were able to achieve good policies in
this game adopted some form of expert imitation and manual
restarts~\cite{bib:mz-openai-demonstrations}. In Section~\ref{sec:non-markov},
we'll investigate the main cause of these difficulties.

% Bridge to rewarding behaviours
As we will see throughout this thesis, a promising solution for these
environments is the construction provided in~\cite{bib:degiacomo-logic-nmrdp}
and~\cite{bib:bolt}. The former work~\cite{bib:degiacomo-logic-nmrdp} has
shown that a Non-Markovian Reward Decision Process (NMRDP) can be easily
declared with linear-time temporal logics, and it has provided a translation
from this NMRDP to a classic MDP. The logics they used are~\ltl{} and~\ldl{}.
This idea was initially introduced in~\cite{bib:nmrdp-logic-first} for a
linear temporal logic of the past. The latter work~\cite{bib:bolt}, instead,
has shown that through same construction, it's possible to declare with
temporal logics the rewards of a RL agent, thus influencing its final
behaviour.  This paper named this additional module with ``Restraining Bolt''.
Thorough this thesis, it might be handy to use this name to refer to this
logic construction.


\section{Objective and results}

\label{sec:intro-objective}

% Two goals
This work started with an initial goal: defining and implementing a Deep
RL agent that, through the Restraining Bolt method, is able to achieve
non-Markovian goals. As we will see, the environments adopted in Deep RL are
much more complex than those of classic RL. They usually have very complex
observation spaces, in which it's much harder to decide the truth of the logic
propositions used in the Restraining Bolt. This general issue required to
investigate new ways to solve the problem of fluents valuation, that we've
opened our introduction with. So, while this thesis achieved the initial goal,
also start a
% TODO: si prova con i fluenti

% TODO: separate high-level objective and specific list of results.
% TODO
%Every choice or assumption that restricts the applicability of
%the proposed method will be pointed our along the text.

% Goal 1: computing fluents
The main purpose of this work is to devise and test a mechanism able to learn
functions which valuate the fluents we define.  Specifically, learn a
function that computes the truth value for a set of boolean conditions, given
a frame of an Atari game. Among the many different ways to accomplish this,
the most interesting techniques are those which pose the least number of
assumptions on the specific environment. In this respect, the following are
important achievements of this work to be highlighted:
\begin{itemize}
	\item Fluents are selected first. Then, the function to evaluate them is
		trained from a description of each fluent. This is harder to do than
		just training a features extractor and manually trying to associate a
		meaning to each feature.
	\item To describe the fluents we use temporal logic over finite traces such
		as \ltl{} and \ldl{}. These are employed as tools to formalize any type of
		temporal constraints the fluents are always expected to satisfy. The use
		of such logics for this purpose can be a really generic approach. This
		thesis is an initial investigation about this possibility. As a
		description of a fluent, we must consider everything that guides the
		training process. So, we will certainly consider other types of hints that
		is useful to include, such as visual hints.
	\item The training algorithm won't require any manual annotation, nor
		labelled datasets at all. The main idea is that, inside the agent, two
		components should coexist: the player and the observer. While the player 
		explores the environment, the observer can be trained from the images
		received, without further intervention.
\end{itemize}

% Goal 2: restraining bolt.
The second goal of this thesis is to demonstrate how such trained features can
be exploited by a Reinforcement Learning agent to solve hard games. Tests will
be conducted on Montezuma's Revenge, a game known to be difficult in this
class~\cite{bib:atari-deepq-nature}. In this thesis:
\begin{itemize}
		\item We provide a flexible implementation of the construction described
			in~\cite{bib:degiacomo-logic-nmrdp}\cite{bib:favorito-thesis}, for
			temporal goals.
		\item A deep agent architecture is proposed to merge the technique above
			for the Deep Reinforcement Learning case.
		\item This implementation is then used to specify a temporal goal in
			\ldl{}, sufficient to guide the agent through hard environments.
\end{itemize}
% TODO: I also contributed to flloat


\section{Structure of the thesis}

The rest of this thesis is structured as follows:
\begin{description}[style=nextline]
	\item[\ref{ch:logics}~--~\nameref{ch:logics}]
		In this chapter, an important formalism that will be used throughout the
		thesis is reviewed. We introduce the reader to concepts such as fluents,
		traces and linear-time temporal logics. Then, we will define the Linear
		Dynamic Logic, which is the specific temporal logic used in this text.
	\item[\ref{ch:rl}~--~\nameref{ch:rl}]
		This chapter presents the second large background of this thesis.  We will
		see Reinforcement Learning from the basic concepts and assumptions, in
		Section~\ref{sec:rl}. Section~\ref{sec:deep-rl} reviews some of the
		advancements of the Deep RL field of last years. Then, in
		Section~\ref{sec:non-markov}, we will analyze what happens when the most
		common assumptions of RL (and of Deep RL) are falsified.
		We present the recent Restraining Bolt method in Section~\ref{sec:rb} and
		we apply it to a Deep RL agent, in Section~\ref{sec:rb-deep-model}, by
		proposing an original model.
	\item[\ref{ch:fluents}~--~\nameref{ch:fluents}]
		Here, we'll see how the agent can be trained to valuate a class of fluents
		to their expected truth. A model for the valuation function will be
		proposed and a training algorithm.
	\item[\ref{ch:atarieyes}~--~\nameref{ch:atarieyes}]
		This chapter presents the software that implemented the concepts presented
		in the previous chapters. I will be first explained from a use
		perspective, then the most interesting implementation details will follow.
	\item[\ref{ch:experiments}~--~\nameref{ch:experiments}]
		This chapter contains experiments and training outcomes for two Atari
		games. Experiments will be finalized to test the effectiveness of learning
		the fluents valuation functions and the capabilities of the ``restrained''
		Deep RL agents.
	\item[\ref{ch:conclusions}~--~\nameref{ch:conclusions}]
		This thesis ends with some final considerations about: the main
		conclusions that can be derived from this work; the strength of this
		approach and its weakness; and its possibilities for improvement.
\end{description}

