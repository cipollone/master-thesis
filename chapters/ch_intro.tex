\chapter{Introduction}

% Intro logic
A classic and important branch of Artificial Intelligence (AI) aims at
developing agents that select their actions through forms of logic reasoning,
such as planning. One of the main advantages of these approaches is that
reasoning proceeds by manipulating \emph{abstractions}. In fact, in logic,
we can define symbols that represent any meaningful event or condition that
should be considered. For example, some propositional symbols might represent
conditions such as ``the door is closed'' or ``I am holding an object'', etc.
We'll also call these atomic propositions with the term ``\emph{fluents}''
(a name that suggests that their truth can change over time).

% Grounding
Reasoning methods based on logics are powerful but they imply one fundamental
ability: at each instant, the agent must be able to decide whether those
propositions are true. This means that all symbols that represent conditions
which happen to be true in the environment, must be true for the agent. This
process is called \emph{grounding} and it is essential in order to make
reasoning relevant for the external environment. Unfortunately, this can be
really hard in complex environments, because the agent's sensors may generate
noisy and multidimensional inputs, that are difficult to interpret.

% Observations in RL
Reinforcement Learning (RL) is a successful field of AI, in which the
agent's goal is to learn a policy that maximizes the rewards received.  We
could argue that RL does not require the valuations just mentioned. Still,
rewards and punishments must be supplied somehow, in response to desirable and
undesirable events. We could consider of providing these feedbacks with
programmed ad-hoc conditions, but this can be easily done just for the
simulations we create. Furthermore, as we will see, complex and non-Markovian
tasks can only be solved through a combination of both RL and logic-based
methods; thus, introducing all the needs of the latter.

% Bridge: goal
With this thesis, we defined and implemented an agent based on temporal logics
and Deep Reinforcement Learning. This combination required to investigate new
ways to solve the grounding problem just described, for a specific class of
environments and fluents.
\enlargethispage{1\baselineskip}
% In Section~\ref{sec:intro-objective}, the goal and
% the achievements of this work will be described more precisely.


\section{Related works}

\label{sec:intro-related}

% RL
Reinforcement Learning (RL) is an area of Machine Learning in which the agent
is trained by sending rewards and punishments in response to its actions.
This technique can be also used unknown environments, where a model of the
dynamics is not available, because the agent learns by trying all actions and
by remembering those that lead to the highest rewards. As we will see in
Chapter~\ref{ch:rl}, most RL algorithms assume that the environment can be
modelled with a Markov Decision Process (MDP). Many learning algorithms exist
in this setting~\cite{bib:rl-book}.

% Deep RL
Neural Networks (NN) have brought new possibilities for RL: in Deep
Reinforcement Learning (Deep RL), the agent employs a neural network as a very
expressive function approximator for the quantities it is trying to
learn~\cite{bib:deep-rl}. For example, the optimal q-value is an important
quantity in RL, that the agents are usually designed to learn from the
observations received. The Deep Q-Network (DQN)
algorithm~\cite{bib:atari-deeprl} is one the first to successfully employ
neural networks in RL. They have shown that a Deep RL agent can be trained
directly from complex observations such as the frames of a video game. Without
any modification, the same agent has been able to learn and reach human-level
performances in many of these games.

% Atari games
Games have always been a classic benchmark for AI algorithms, because they
provide various levels of complexity, they have few and strict rules, and they
are easy to implement and simulate. Regarding Deep RL, many authors have
tested their algorithms on the collection of video games
``Atari~2600''~\cite{bib:atari-games}. In this thesis, we'll use and
experiment with games from this collection.

% Algorithms
The reinforcement learning algorithm we've adopted is called Double
DQN~\cite{bib:double-q}. The motivation of this choice is that this is a
relatively simple algorithm, based on DQN, which has also proven to be
successful for the specific environments that we'll use in our
experiments~\cite{bib:atari-deepq-nature}. In fact, among Q-Network
algorithms, the only ones that were able to clearly achieve superior
performances in most of these games have combined many of the others DQN
variants~\cite{bib:rainbow}.

% Some games are hard
If we look at the results in~\cite{bib:atari-deepq-nature}, DQN agents are
able to learn excellent policies for many games. However, for many other
environments of the same collection, the agents struggle to learn and, in some
cases, it doesn't learn anything at all. The worst performances have been
measured for the \emph{Montezuma's Revenge} environment. Even in the works
that followed, the only methods that were able to achieve good policies in
this game adopted some form of expert imitation and manual
restarts~\cite{bib:mz-openai-demonstrations}. In Section~\ref{sec:non-markov},
we'll investigate the main cause of these difficulties.

% Bridge to rewarding behaviours
As we will see throughout this thesis, a promising solution for these
environments is the construction provided in~\cite{bib:degiacomo-logic-nmrdp}
and~\cite{bib:bolt}. The former work~\cite{bib:degiacomo-logic-nmrdp} has
shown that a Non-Markovian Reward Decision Process (NMRDP) can be easily
declared with linear-time temporal logics, and it has provided a translation
from this NMRDP to a classic MDP. The logics the authors used are~\ltl{}
and~\ldl{}.  This idea was initially introduced
in~\cite{bib:nmrdp-logic-first} for a linear temporal logic of the past.

The latter work~\cite{bib:bolt}, instead, has shown that through a similar
construction, it is possible to influence the agent's optimal behaviour by
declaring additional non-Markovian rewards that are produced in conjunction
with the original rewards. This paper named this additional module with
``Restraining Bolt''.  Thorough this thesis, it may be practical to use this
name to refer to this logic construction.


\section{Objective and results}

\label{sec:intro-objective}

% Two goals
This work started with the following goal: defining and implementing a Deep RL
agent that, through the Restraining Bolt, is able to achieve non-Markovian
tasks. This first objective has been accomplished, but the Restrained Bolt is
a method based on logic, and, as we've already anticipated in this
introduction, it requires to correctly valuate the fluents we define. Since
the environments adopted in Deep RL have much more complex observation spaces,
it has been necessary to investigate new ways to solve the problem of the
fluents valuation, at least for a specific class of fluents and observations.

%  Agent
Thus, it is possible to isolate two groups of contributions of this thesis:
those concerning the definition of a Deep RL agent for non-Markovian goals,
and those related to methods for learning the fluents' valuation function.
Regarding the former, in this thesis:
\begin{itemize}
		\item We provide a flexible implementation of the Restraining Bolt method
			described in~\cite{bib:bolt} and~\cite{bib:degiacomo-logic-nmrdp}.
		\item We proposed a Neural Network architecture for a Deep RL agent that
			allowed to successfully employ the Restraining Bolt also in Deep
			Reinforcement Learning.
		\item Tests have been conducted in a video game called \emph{Montezuma's
			Revenge}, the hardest game (for a RL agent) in the Atari~2600
			collection~\cite{bib:atari-deepq-nature}. We've shown that the agent can
			be successfully guided through the initial room of the game, with low
			manual intervention.
\end{itemize}

% Fluents
The latter topic, instead, received much attention in this thesis: how is it
possible to learn a function that, given an observation of the environment,
predicts whether the fluents we have defined are true? As we may recognise,
this problem is really general, and we must restrict to a specific class of
fluents and observations. Observations will be frames of the Atari games, and
fluents will be propositions decidable from each of these images. Every choice
or assumption that will further restrict the applicability of the proposed
method will be pointed our along the text. Still, there are some interesting
achievements of this work that are to be highlighted:
\begin{itemize}
	\item We don't manually assign a meaning to some Boolean features.
		Instead, this is an initial investigation about how to proceed in the
		opposite direction: we first define a set of symbols, then we learn their
		valuation function.
	\item We adopt the temporal logic \ldl{} as a formalism to define some
		temporal constraints that our fluents are always expected to satisfy.
		Candidate valuation functions will be checked against these constraints.
	\item The training algorithm won't require any manual annotation,
		nor labelled datasets at all.
	\item We propose an architecture, based on Deep Belief Networks, that by
		reducing the input space dimensionality, binds the valuations to some
		visual features that are recognizable in the image.
\end{itemize}
All these ideas have been implemented in a Python software and tested on games
of the Atari collection.

Finally, we also strongly contributed to the development of the \code{flloat}
package~\cite{bib:flloat}, a Python software for the conversion of \ltl{} and
\ldl{} formulae to the associated automaton (NFA or DFA).


\section{Structure of the thesis}

The rest of this thesis is structured as follows:
\begin{description}[style=nextline]
	\item[\ref{ch:logics}~--~\nameref{ch:logics}]
		Temporal logics are an important formalism for this work and will be used
		throughout the text. This chapter introduces the reader to concepts such
		as: fluents, traces and linear-time temporal logics. Then, we will define
		the Linear Dynamic Logic~(\ldl{}), that is the specific temporal logic
		used in this text.
	\item[\ref{ch:rl}~--~\nameref{ch:rl}]
		Here we describe the second large group of background topics for this
		thesis. We introduce the fundamental concepts of RL, Deep RL, and we
		define Double DQN, the Deep RL algorithm that we'll use.
	\item[\ref{ch:nonmarkovrl}~--~\nameref{ch:nonmarkovrl}]
		This chapter describes how to design RL agents able to achieve
		non-Markovian goals. Here, we'll see an in-depth description of the
		problem, we'll present recent solution methods and also propose some
		original ideas. In Section~\ref{sec:non-markov}, we will analyze what
		happens when the most common assumptions of RL, and of Deep RL, are
		falsified. A recent solution for these complex RL problems will be
		presented in Section~\ref{sec:non-markov-solutions}, where we explain the
		Restraining Bolt method. This section also aims to illustrate two sources
		of non-Markovian rewards in a unified discussion, and to formalize the
		concept of Restraining Bolt applied to a class of POMDPs. The chapter ends
		with Section~\ref{sec:rb-deep-model}, where we propose an original model
		that allows to apply the Restraining Bolt method also in the context of
		Deep RL.
	\item[\ref{ch:fluents}~--~\nameref{ch:fluents}]
		In this important chapter, we illustrate the proposed training method for
		the fluents valuation functions. We also define a specific model that
		encodes a portion of the input frame to generate the fluent Boolean
		valuations.  We'll also formulate the appropriate training algorithm for
		this model.
	\item[\ref{ch:atarieyes}~--~\nameref{ch:atarieyes}]
		This chapter presents the software that implements all the concepts
		presented in the previous chapters. We will first review its features and
		its functionality from a user perspective. Then, the most interesting
		implementation details will follow.
	\item[\ref{ch:experiments}~--~\nameref{ch:experiments}]
		This chapter contains experiments and training outcomes in two Atari
		games. The experiments will be finalized to test: the effectiveness of the
		proposed method for learning the valuation functions; the combination of
		such computed fluents in a Deep RL agent with the Restraining Bolt.
	\item[\ref{ch:conclusions}~--~\nameref{ch:conclusions}]
		In this final chapter, we draw the main conclusions that can be derived
		from this work, the strength of this approach and its weakness, and many
		of the possibilities for improvement.
\end{description}

