\chapter{Deep Reinforcement Learning for non-Markovian goals}

\section{Reinforcement Learning}

% Motivation
In this section we will briefly review the most important aspects of classic
\emph{Reinforcement Learning} (RL). These concepts are relevant because they
are also found in Deep Reinforcement Learning (Deep RL), which is a central
component of the agent we will design. Excellent references for these topics
are \cite{bib:rl-book}, \cite{bib:probabilistic-rl}, and
\cite{bib:ml-book-murphy} for graphical models.

% Agent-env interface
In AI, we commonly isolate two entities, the agent and the environment, which
continuously interact. At each instant, the agent receives observations from
the environment and it executes actions in response. In RL specifically, the
agent observes the current state of the environment and a numerical reward.
The environment produces high rewards in response to desirable events. The
agent's goal is to maximize the rewards received. The basic setup is
illustrated in Figure~\ref{fig:rl}.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\node [block] (agent) {Agent};
		\node [block, below=of agent] (env) {Environment};
		\draw [flow] ([yshift=2mm]env.west) -- ++(-1.2,0) |-
			([yshift=-2mm]agent.west) node [pos=0.3,right] {reward\\$r$};
		\draw [flow] ([yshift=-2mm]env.west) -- ++(-1.5,0) |-
			([yshift=2mm]agent.west) node [pos=0.3,left] {state\\$s$};
		\draw [flow] (agent.east) -| node [pos=0.7,right] {action\\a}
		($(env.east)+(1.2,0)$) -- (env.east);
	\end{tikzpicture}
	\caption{How agent and environment interact in RL.}
	\label{fig:rl}
\end{figure}

% MDP
Most RL algorithms assume that the environment dynamics can be modelled with a
\emph{Markov Decision Process} (MDP). They do so, because under the
independence assumptions taken by MDP, it's possible to efficiently find
the optimal agent's policy. A Markov Decision Process is a tuple $\langle S,
A, T, R, \discount \rangle$, where: $S$ is the set of states of the
environment; $A$ is the action space; $T: S \times A \times S \to \R$ is the
transition function, which, for ${T(s_{t}, a_{t}, s_{t+1})}$,
returns the probability $p(s_{t+1} \given s_{t}, a_{t})$ of
the transition ${s_{t} \xrightarrow{a_{t}} s_{t+1}}$; $R: S
\times A \times S \to \R$ is the reward function; and $\discount \in [0, 1]$
is called ``discount factor''\footnote{In this chapter, subscripts with whole
numbers or $t$ indicate the value of the variable at the discrete time
indicated.}.

% Markov assumptions
In a RL problem, the functions $T$ and $R$ are unknown. The agent can only
learn them by observing the samples it receives from the environment.
However, by assuming that they can be modelled with functions ${S \times A
\times S \to \R}$, we introduce some Markov assumptions. In particular, we
assume that the next state of the environment is conditionally independent on
the whole history, given the previous state and action: $s_{t+1} \perp
s_{0}, \dots, s_{t-1} \given s_{t}, a_{t}$. Similarly,
the reward only depends on the last transition of the environment.  Although
it's not required by the model, it is common that rewards are computed just
from desirable configurations of the environment~$s_{t}$, not from
specific transitions $(s_{t}, a_{t}, s_{t+1})$. All of these
assumptions are summarized in the directed graphical model of
Figure~\ref{fig:mdp}. The lack of any arrow between $s_{t-1}$ and
$s_{t+1}$ indicates that future states, hence the rewards, do not depend
on the past history. This is the essence of a Markov assumption.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\matrix [
			column sep={1cm,between origins}, row sep={1cm,between origins},
		] {
			\&
			\node (am1) [node, label=above:$a_{t-1}$] {};
			\& \&
			\node (a) [node, label=above:$a_{t}$] {}; \& \\
			\node (stm1) [node, label=above:$s_{t-1}$] {};
			\& \&
			\node (st) [node, label=above:$s_{t}$] {}; \& \&
			\node (st1) [node, label=above:$s_{t+1}$] {}; \\
			\& \&
			\node (r) [node, label=below:$r_{t}$] {}; \& \&
			\node (r1) [node, label=below:$r_{t+1}$] {}; \\
		};
		\draw (st1) edge [<-] (st) edge [<-] (a);
		\draw [->, dotted] (st) -- (a);
		\draw [->] (st) -- (r);
		\draw [->] (st1) -- (r1);
		\draw (st) edge [<-] (stm1) edge [<-] (am1);
		\draw [->, dotted] (stm1) -- (am1);

		\draw [dashed, gray] (st1) -- +(0.8,0);
		\draw [dashed, gray] (stm1) -- +(-0.8,0);
	\end{tikzpicture}
	\caption{The directed graphical model of a MDP.}
	\label{fig:mdp}
\end{figure}

The \emph{policy} is the criterion the agent uses to select the actions to
perform. If the environment dynamics can be modelled with a MDP, the
optimal action at time~$t$ only depends on~$s_{t}$. So, there must exist
an optimal policy as $\policy^*: S \to A$. Due to common estimation errors, it
is always better to prefer nondeterministic policies, which return a
probability distribution over the actions. The action at time $t$ will be
sampled according to $a_t \sim \policy(s_t)$. This dependency is represented
by the dotted arrow of Figure~\ref{fig:mdp}.

We will now introduce few basic quantities of RL that serve to define what it
means for an action or a policy to be optimal. The \emph{discounted
return}~$G$ is the combination of all rewards collected:
\begin{equation}
	G \coloneqq r_{0} + \discount\, r_1 + \discount^2 r_2 + \dots =
	\sum_{t=0}^{T} \discount^{t} r_{t}
	\label{eq:return}
\end{equation}
The discount factor, $0 \le \discount \le 1$, decides the relative importance
of immediate and future rewards. Usually, this factor is strictly less than 1
because this stimulates the agent to achieve rewards as soon as possible.
It also produces a finite discounted reward, even for an infinite run $T \to
\infty$.  Since our environments are video games, each play is an episode and
the total number of steps in each episode is finite.

It is now clear, that the optimal policy should always maximize the expected
discounted return. The \emph{value function} of a policy $\policy$ computes
this quantity for each state $s$:
\begin{equation}
	v_{\policy}(s) \coloneqq \E_{\policy}[G \given s_0 = s]
\end{equation}
which computes the expected value of $G$, when the agent starts from state $s$
and it follows the policy~$\policy$. The notation $\E_{\policy}$ indicates
that the estimation assumes that the actions are sampled according
to~$\policy$. Finally, we can define the optimal policy $\policy^*$ as the one
maximizing the value function at all states:
\begin{equation}
	\policy^*: \quad v_{\policy^*}(s) \ge v_{\policy}(s) \qquad \forall s \in S,
	\quad \text{for all $\policy$}
\end{equation}
The typical Reinforcement Learning problem is to find the optimal policy for
an MDP with unknown $T$ and~$R$.

The \emph{action-value function} of a policy $\policy$ is a similar measure to
the value function:
\begin{equation}
	q_{\policy}(s, a) \coloneqq \E_{\policy}[G \given s_0 = s, a_0 = a]
\end{equation}
which forces the first action to be~$a$. Since the only evidence the agent can
collect are outcomes of single actions, this is usually a much more convenient
form for updating the estimate of the expected discounted return.

If we could know the 

% TODO We'll only deal with discrete action spaces, where the policy
% is a categorical distribution (a vector of probabilities).



\section{Temporal logics and Linear Dynamic Logic}

\subsection{Temporal logics on finite traces}

% Intro to temporal logics
Temporal logics are a class of formal languages, more precisely modal logics,
that allow to talk about time~\cite{bib:temporal-logics-stanford}. Among all
formalisms, we care about logics that assume a linear time, as opposed to
branching, and a discrete sequence of instants, instead of continuous time.
In computer science, the most famous logic in this group is the Pnueli's
Linear Temporal Logic (LTL)~\cite{bib:pnueli-ltl}.

% Structures
The assumptions about the nature of time directly reflect to the type of
structures these logics are interpreted on: their models are $\modelsym =
\langle T, \prec, V \rangle$, where the set $T$ is a discrete set of time
instants, such as $\N$, and $\prec$ is a complete ordering relation on $T$,
like~$<$. If a logic defines a set $\fluents$ of atomic propositions, the
evaluation function $V: T \times \fluents \to \{\true, \false\}$, for each
instant of time, assigns a truth value to each fluent. An equivalent and
compact way of defining such structures is with \emph{traces}. A trace
$\trace$ is a sequence of propositional interpretations $2^\fluents$ of the 
fluents~$\fluents$. Each element of the trace, $\trace(i)$, is the set of true
symbols at time~$i$. $\trace(i, j)$ represents the trace between instants $i$
and~$j$.
% TODO: evaluation or valuation?

% Structures in LTL
LTL is a logic that only allows to talk about the future. The semantic of its
temporal operators, neXt~$\next$, Until~$\until$, and of those derived,
eventually~$\eventually$, always~$\always$, can only access future instants on
the sequence. Interpretations for this logic are infinite traces with a first
instant, which are equivalent to valuations on the temporal frame $\langle \N,
< \rangle$.

% Finite traces
As it has been pointed out~\cite{bib:ltlf-ldlf}, most practical uses of LTL
interpret the formulae on \emph{finite} traces, not infinite. The pure
existence of a last instant of time has strong consequences on the meaning of
the operators, because they need to handle such instant differently. The
Always operator~$\always$, translates to ``until the last instant'', quite
naturally. However, writing $\always\eventually \formula$ does not require
that $\formula$ becomes true an infinite number of times, that is the
``response'' property; instead, it is satisfied exactly by those traces in
which $\formula$ is true at $\const{Last}$ ($\const{Last}$ is an abbreviation
for $\lnot \next \true$ and it evaluates to true at last instant only).
Furthermore $\always\eventually \formula$ and $\eventually\always \formula$
are both equivalent to $\eventually (\const{Last} \land \formula)$, something
that doesn't happen in standard LTL. From last example, it should be clear
that the expressive power of the language has changed and LTL interpreted over
finite traces should be regarded as a different logic, that we denote with
\ltl{}. More precisely, over infinite linearly-ordered interpretations, LTL
has the same expressive power of Monadic Second Order Logic (MSO), while
\ltl{} is equivalent to First-Order Logic (FOL) and star-free regular
expressions, which are strictly less expressive.

% Every finite is fine
In the next section, we will define a temporal logic, called \ldl{}, that is
purposefully devised for finite traces. This is the formalism that we use in
the implemented construction for RL agents. However, many plans and behaviours
to be rewarded can be also expressed with~\ltl{}. So, for this construction,
any temporal logic over finite traces which can be translated to equivalent
finite-state automata can be used as an alternative to~\ldl{}; even temporal
logics of the past~\cite{bib:nmrdp-logic-first}.


\subsection{Linear Dynamic Logic}

In this section, we will define Linear Dynamic Logic of finite traces
(\ldl{})~\cite{bib:ltlf-ldlf}. Its syntax combines regular expressions and
propositional logic, just like Propositional Dynamic Logic (PDL)
does~\cite{bib:pdl}\cite{bib:pdl-stanford}. So, we will review regular
expressions first.


\subsubsection{Regular Temporal Specifications}

Regular languages are the class of languages exactly recognized by finite
state automata and regular expressions~\cite{bib:languages-book}. So, we will
use regular expressions as a compact formalism to specify them. Regular
expressions are usually said to accept strings. Traces are in fact strings,
whose symbols $s \in 2^{\fluents}$ are propositional interpretations of the
fluents~$\fluents$. Such regular expressions would be:
\begin{equation}
	\resym ::= \emptyset \mid s \mid
	\resym_1 + \resym_2 \mid \resym_1 ; \resym_2 \mid \resym^*
	\label{eq:re-no}
\end{equation}
where $\emptyset$ denotes the empty language, $s \in 2^\fluents$ is a symbol,
$+$ is the disjunction of two constraints, $;$ separates concatenated
expressions, and $\resym^*$ requires an arbitrary repetition on $\resym$.
Parentheses can be used to group expressions with any precedence.

We call the regular expressions of equation~\eqref{eq:re-no} Regular Temporal
Specifications \re{}, because they are interpreted on finite linear temporal
structures. However, writing specifications in terms of single interpretations
is very cumbersome. So, we substitute the symbols $s \in 2^\fluents$ with
formulae of Propositional Logic. A propositional formula $\propformula$
represents all interpretations that satisfy it: $\text{Sat}(\propformula) = \{s
\in 2^\fluents \mid s \models \propformula\}$.

The new definition for the syntax of Regular Temporal Specifications \re{}:
\begin{equation}
	\resym ::= \propformula \mid
	\resym_1 + \resym_2 \mid \resym_1 ; \resym_2 \mid \resym^*
	\label{eq:re}
\end{equation}
where $\propformula$ is a propositional formula on the set of atomic
symbols~$\fluents$. The language generated by a \re{}~$\resym$, denoted
$\langsym(\resym)$, is the set of traces that match the temporal
specification. The only difference with regular expressions' standard
semantics is that a symbol $s \in 2^\fluents$ matches a propositional formula
$\propformula$ if and only if $s \in \text{Sat}(\propformula)$. A trace that
match the regular expression $\trace \in \langsym(\resym)$ is said to be
generated or accepted by the specification~$\resym$.

\begin{example}
	As an example, let's define a \re{} expression $\resym \coloneqq \true;
	(\lnot B)^*; (A \land B)$ and the following traces:
	\begin{align*}
		\trace_1 &\coloneqq \langle \set{}; \set{A}; \set{A}; \set{A,B} \rangle \\
		\trace_2 &\coloneqq \langle \set{B}; \set{A,B} \rangle \\
		\trace_3 &\coloneqq \langle \set{A, B}; \set{B}; \set{B} \rangle \\
	\end{align*}
	The first two traces are accepted by the expression, $\trace_1, \trace_2 \in
	\langsym(\resym)$, but the third is not, $\trace_3 \not\in
	\langsym(\resym)$. Of course, the symbols $A$ and $B$ could represent any
	meaningful property of the environment to be ensured.
\end{example}


\subsubsection{Linear Dynamic Logic}

Linear Dynamic Logic is a temporal logic for finite traces that was first
defined in~\cite{bib:ltlf-ldlf}. The definition we see here, also adopted by
the implementation we'll use, is a small variant that can also be interpreted
over the empty trace, $\trace_\epsilon = \langle \rangle$, unlike most logics
that assume a non-empty temporal domain~$T$.

\begin{definition}
	A \ldl{} formula $\formula$ is built as follows:
	\begin{equation}
	\begin{aligned}
		\formula \quad &::= \quad \ltt \mid \lnot \formula \mid \formula_1 \land
			\formula_2 \mid \langle \resym \rangle \formula \\
		\resym \quad &::= \quad \propformula \mid \formula? \mid \resym_1 +
			\resym_2 \mid \resym_1; \resym_2 \mid \resym^* \\
	\end{aligned}
	\label{eq:ldl-syntax}
	\end{equation}
	where $\ltt$ is a constant that stands for logical true and $\propformula$
	is a propositional formula over a set of symbols~$\fluents$.
\end{definition}

The syntax just defined is really similar to PDL~\cite{bib:pdl}, a well known
and successful formalism in Computer Science for describing states and events
of programs. However, \ldl{} formulae are interpreted over finite traces
instead of Labelled Transition Systems.

Before moving to the semantics, we can intuitively understand the meaning of
the constructs. The second line of definition~\eqref{eq:ldl-syntax} is a
Regular Temporal Specification \re{}, with the addition of the test
operator~$?$, typical of PDL. In $\langle \resym \rangle \formula$, the \re{}
expression $\resym$ is used as a modal operator to move to future states: it
states that there exists at least one 

\section{Reinforcement learning with restraining specifications}
Restraining Bolt method~\cite{bib:bolt}\cite{bib:rb-imitation-l}.

