\chapter{Learning to valuate fluents in games}

\label{ch:fluents}

In Chapter~\ref{ch:rl}, we've presented the problem of Deep Reinforcement
Learning for non-Markovian rewards. We've seen that a construction based on
temporal logics, that we call Restraining Bolt, is an elegant solution that,
by producing additional rewards and observations, transforms the original
problem to a classic MDP. We report the general scheme here, in
Figure~\ref{fig:rb-focus-features}.
\begin{figure}
	\centering
	\begin{tikzpicture}[
			every node/.append style={font=\small},
			arrow/.style={->, semithick},
		]
		\matrix [
			 row sep=0.5cm, column sep=1cm,
		] {
			\node (env) [block, minimum size=2cm] {World}; \& \& \&
			\node (agent) [block, minimum size=2cm] {Learning\\agent}; \\
			\&
			\node (features) [block, fill=lightgray] {Features\\extractor}; \&
			\node (rb) [block] {Restraining\\Bolt}; \& \\
		};
		\coordinate (a1) at ($(agent.west)+(0,0.5)$);
		\draw [arrow] (a1) -- node [near start, above] {$a$} (env.east |- a1);
		\coordinate (w1) at ($(env.east)+(0,-0.2)$);
		\draw [arrow] (w1) -- node [near start, above] {$r$} (agent.west |- w1);
		\coordinate (w2) at ($(env.east)+(0,-0.5)$);
		\draw [arrow] (w2) -- node [near start, below] {$o$} (agent.west |- w2);
		\draw [arrow] (w2) ++(0.5,0) |- (features.west);
		\draw [arrow] (features) -- node [near start, above] {$l$} (rb);
		\draw [arrow] ($(rb.east)+(0,0.1)$) -- ++(0.3,0)
			|- node [below right] {$\vec{q}$} ($(agent.west)+(0,-0.8)$);
		\draw [arrow, dashed] ($(rb.east)+(0,-0.1)$)
			-| node [near end,right] {$r'$} (agent.south);
	\end{tikzpicture}
	\caption{In this chapter, we focus on the features extractor.}
	\label{fig:rb-focus-features}
\end{figure}
The two blocks at the bottom are our additions, and part of the solution.
We've thoroughly addressed the Restraining Bolt in Section~\ref{sec:rb},
already. In this chapter we want to focus on the other essential component:
the features extractor.

The purpose of the features extractor is to receive an observation from the
environment and produce a Boolean valuation for some predefined propositional
symbols, that we call fluents. We assume that the set of fluents~$\fluents$
has already been defined, and the truth of every atomic proposition
in~$\fluents$ can be decided from a single input~$o$~\footnote{If we did
define a symbol that cannot be decided, for example a generic
``$\const{GoalReached}$'', we need to split that condition into much simpler
events, and define $\const{GoalReached}$ in terms of the new symbols.}.

Usually, the features extractor is not a really interesting component. Once,
we've defined a fluent~$p \in \fluents$, we could manually program a function,
$f_p: \obsS \to \B$, that predicts when that event occurs or that condition is
verified\footnote{$\B$ is the set $\set{0, 1}$, which represents $\set{\false,
\true}$.}.  This approach is perfectly fine, when applicable.  However, the
environments used in Deep Reinforcement Learning usually produce
high-dimensional or noisy observations. As we may imagine, it becomes really
hard to manually classify such inputs in the two classes. So, in order to
apply to Deep RL the Restraining Bolt, or any other logic-based method, we
must resort to some Machine Learning model that will help us deciding the
truth of our atomic propositions.

Any Deep RL agent processes the input observation with a Neural Network. A
reasonable choice would be to use a NN also as model for the features
extractor. We may use a joint network that predicts the value for every fluent
defined: $f: \obsS \to \B^{\abs{\fluents}}$. The simplest way to train this
model is through supervised learning, where we provide many input-output
samples. Supervised learning can generate very accurate models, but, for every
image in the training dataset, we would need to manually label the desired
outputs, i.e. the fluents that are true in that image. The effort of this
manual intervention would completely dominate over the advantages of the
high-level, logic approach. If possible, we would certainly like to avoid this
manual work. 

A very promising alternative is unsupervised learning. These models don't
return predictions. Instead, they memorize patterns and features that the
training inputs have in common. These models have two representations: the
input space, and the latent space. To any input that is presented to the
model corresponds a compact representation in the latent space. The purpose
of this representation is to distinguish the specific input among all of the
training set\footnote{To emphasize this concept: the purpose of the latent
representation is to distinguish the input sample with respect to the training
distribution.}. Since the latent space is much more compact, it may be used in
other computations in place of the original input. In this case, the latent
vector is called an \emph{encoding}.

Unsupervised learning will be a central part of the solution proposed here.
However, it may not be the only part, because unsupervised models makes no
guarantees about the meaning of the latent representation. This means that
we cannot predict what each number in the latent vector represents. So, the
proposed model transforms the encodings through a second function, which
computes the truth value of the fluents. 


\section{Temporal constraints}

In this whole Chapter, we'll develop an original technique to realize the
features extractor. These are the general goals that we want to pursue with
this work:
\begin{itemize}
	\item We choose the set of fluents that should be learnt.
	\item Learning without manual labelling.
	\item As few environment specific choices as possible.
\end{itemize}

In this section, we illustrate the concept of ``temporal constraints''. This
is the central idea introduced with this work, that will help us achieve
the three principles above. As we'll discuss in
Section~\ref{sec:fluents-assumptions} and~\nameref{sec:fluents-limitations},
this method, as realized in this thesis, makes some assumptions that limits
its applicability to a specific class of fluents and observations. However, it
poses some interesting ideas that certainly needs to be further investigated
in future research. This thesis is just an initial study in this direction.

We can start from the following observation: a dataset of labelled samples is
a description, by examples, of the desired meaning of the fluents. A good
model would interpolate between these samples to inputs that have never been
observed. Without these examples, how do we specify the desired meaning of a
fluent? Note that by ``meaning'', we mean the set of inputs in which the
propositional symbol should be valuated to true.

What we propose here is to specify the desired temporal behaviour of these
fluents with temporal logics: we can write a temporal formula, in \ltl{} or
\ldl{}, that describes all the possible traces of the fluents we want to
define. We don't talk about \emph{desirable} trajectories. Instead, we define
all the \emph{possible} trajectories according to the environment dynamics.
For example, suppose that two conditions $A$ and $B$ cannot be true at
the same time. Regardless of what we're trying to achieve, we can write the
following temporal constraint: $\lbox{\true^*}(\lnot A \lor \lnot B)$.
This constraint is an invariant because it should hold in every instant, but
there are many other interesting constraints that we may specify with temporal
logic. We have $A$ and $\ldiamond{\true^*}(\llast \land A)$, which
respectively mean: every episode starts/ends with $A$ that is $\true$.
Also, $\lbox{\true^*; A}\ldiamond{\true^*}B$ means that every time $A$ becomes
true, the event $B$ must follow later on. This is a frequent pattern in
request--response behaviours. The automaton associated to this constraint is
shown in Figure~\ref{fig:response-automa}.
\begin{figure}
		\centering
		\begin{tikzpicture}
		\graph [
			automaton, grow right=3cm,
		]{
			0 [accept] -> ["$A$"] 1;
			0 -> [self loop, "$\lnot A$"] 0;
			1 -> [self loop, "$A \lor \lnot B$"] 1;
			1 -> [backward, "$B \land \lnot A$"] 0;
		}; 
		\draw [init path] (0.west) +(-0.5,0) -- (0.west);
		\end{tikzpicture} 
		\caption{The DFA associated to the formula ${\lbox{\true^*;
			A}\ldiamond{\true^*}B}$.}
		\label{fig:response-automa}
\end{figure}
% TODO: note about symbolic transitions?

\begin{example}
	Suppose that an agent should open a door that is closed with a key, and
	we've defined the fluents $\fluents \coloneqq \set{\const{HaveKey},
	\const{DoorOpen}}$. We need to train a feature extractor that valuates these
	two propositions with their intended meaning. We may write the following
	constraint:
	\[
		(\lnot \const{HaveKey} \land \lnot \const{DoorOpen})
		\land \lnot \ldiamond{\true^*; \lnot \const{HaveKey}}\const{DoorOpen}
	\]
	which says that the door cannot be opened if at the previous instant we
	don't have a key, and initially the door is closed and the agent has no key.
	The automaton associated is shown in Figure~\ref{fig:door-automa}.
	\begin{figure}
			\centering
			\begin{tikzpicture}
			\graph [
				automaton, grow right=5cm,
			]{
				0 [accept] -> {
					1 [accept, >"$\lnot\const{DoorOpen} \land \lnot\const{HaveKey}$"] %
						-> ["$\const{HaveKey} \land \lnot\const{DoorOpen}$"] %
						2 [accept],
					3 ["$\const{DoorOpen} \lor \const{HaveKey}$"]
				};
				3 -> ["$\true$", self loop] 3
			}; 
			\draw [init path] (0.west) +(-0.5,0) -- (0.west);
			\end{tikzpicture} 
			\caption{The DFA associated to Example~\ref{ex:door}.}
			\label{fig:door-automa}
	\end{figure}
	% TODO
	Note that we didn't specify that the door should be eventually opened. The
	automaton only excludes the trajectories that certainly can't happen.
	% TODO: not complete, because no planning, just constraints
	\label{ex:door}
\end{example}

We've just described how temporal constraints work. However, there is one
important 

% TODO: not alone
% TODO: only binding fluents together, not to observations


\section{Assumptions}

\label{sec:fluents-assumptions}

A temporal constraints aren't definitions; they are just minimal constraints.
We need additional clues: visual description of fluents.
Now follow my assumptions:
\begin{itemize}
	\item Local propertes (with regions I don't have to find elements in a
		frame).
	\item The property is visually apparent, inside the region.
\end{itemize}

Limitations and other ideas for a stronger grouding.

\section{General structure of the model}

Illustration and general description of the model.

\section{Encoding}

Encoder: the model, how it works, what does it learn, size of the encoding.

References:
Training Restricted Boltzmann Machines and Deep Belief Neworks
\cite{bib:rbm-training}\cite{bib:ml-book-murphy}.

\subsection{Model: Deep Belief Network}

\subsection{What does it learn}


\section{Boolean functions}

The fluents are true in a set of those configurations.

\subsection{Learning with genetic algorithms}

Ideas from concept learning; genetic algorithm.

References:
Genetic Algorithms for Concept learning\cite{bib:ga-for-concepts},
Genetic Algorithms review\cite{bib:ga-mutations-review}.

\subsection{Boolean rules}

Representation of boolean functions and training details.

\section{Limitations and improvements}

\label{sec:fluents-limitations}

% TODO: improve: maybe also some labels?
% TODO: over temporal constraints alone

% TODO: unsure about representable with conjunction, because DBN is
% unsupervised
