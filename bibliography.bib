@book{bib:aima,
author = {Russell, Stuart and Norvig, Peter},
title = {Artificial Intelligence: A Modern Approach},
year = {2009},
isbn = {0136042597},
publisher = {Prentice Hall Press},
address = {USA},
edition = {3rd}
}
@article{bib:deep-rl,
abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
archivePrefix = {arXiv},
arxivId = {1811.12560},
author = {Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle},
doi = {10.1561/2200000071},
eprint = {1811.12560},
isbn = {9781680833683},
issn = {1935-8237},
journal = {Foundations and Trends in Machine Learning},
month = {11},
number = {3-4},
pages = {219--354},
title = {{An Introduction to Deep Reinforcement Learning}},
volume = {11},
year = {2018}
}
@article{Rabinovich1998a,
abstract = {The paper compares the expressive power of monadic second order logic of order, a fundamental formalism in mathematical logic and theory of computation, with that of a fragment of Temporal Logic of Actions introduced by Lamport for specifying the behavior of concurrent systems. {\textcopyright} Springer-Verlag Berlin Heidelberg 1998.},
author = {Rabinovich, Alexander},
doi = {10.1007/bfb0055772},
isbn = {3540648275},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
number = {January 1998},
pages = {229--238},
title = {{Expressive completeness of temporal logic of action}},
volume = {1450 LNCS},
year = {1998}
}
@article{Cipollonea,
author = {Cipollone, Roberto},
pages = {1},
title = {{Note riguardo Restraining Bolts}}
}
@article{Gurfinkel,
author = {Gurfinkel, Arie},
title = {{LTL - Linear Time Logic ( Pn 77 )}}
}
@article{Peled1997,
abstract = {We show that every stutter-invariant prepositional linear temporal property is expressible without the next-time operator. {\textcopyright} 1997 Elsevier Science B.V.},
author = {Peled, Doron and Wilke, Thomas},
doi = {10.1016/s0020-0190(97)00133-6},
issn = {00200190},
journal = {Information Processing Letters},
keywords = {Next operator,Specification languages,Stutter invariance,Temporal logic},
number = {5},
pages = {243--246},
title = {{Stutter-invariant temporal properties are expressible without the next-time operator}},
volume = {63},
year = {1997}
}
@article{Saffidine2009,
abstract = {Linear Temporal Logic (LTL) Model Checking can be used to check whether a concurrent system satisfies constraints such as fairness or liveliness among others. The main bottleneck is the space taken by the structure used to represent the system. When the LTL formula does not contain the $\backslash$emph{\{}next{\}} operator, partial order reduction can be used to reduce the space requirement. We tried in this internship to use LTL fragments to be able to reduce the space requirement when the formula does contain $\backslash$emph{\{}next{\}}, or to reduce even more the space requirement when the formula does not contain $\backslash$emph{\{}next{\}} and contains a bounded number of $\backslash$emph{\{}until{\}}.},
author = {Saffidine, Abdallah},
number = {May},
title = {{LTL Model Checking with use of Generalised Stuttering and Characteristic Patterns}},
year = {2009}
}
@article{bib:bolt,
abstract = {In this work we investigate on the concept of "restraining bolt", envisioned in Science Fiction. Specifically we introduce a novel problem in AI. We have two distinct sets of features extracted from the world, one by the agent and one by the authority imposing restraining specifications (the "restraining bolt"). The two sets are apparently unrelated since of interest to independent parties, however they both account for (aspects of) the same world. We consider the case in which the agent is a reinforcement learning agent on the first set of features, while the restraining bolt is specified logically using linear time logic on finite traces LTLf /LDLf over the second set of features. We show formally, and illustrate with examples, that, under general circumstances, the agent can learn while shaping its goals to suitably conform (as much as possible) to the restraining bolt specifications.},
author = {{De Giacomo}, Giuseppe and Iocchi, Luca and Favorito, Marco and Patrizi, Fabio},
isbn = {9781577358077},
issn = {23340843},
journal = {Proceedings International Conference on Automated Planning and Scheduling, ICAPS},
number = {Brooks 1991},
pages = {128--136},
title = {{Foundations for restraining bolts: Reinforcement learning with LTLf/LDLf restraining specifications}},
year = {2019}
}
@inproceedings{bib:rbm-persistent-cd,
abstract = {A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple. Copyright 2008 by the author(s)/owner(s).},
address = {New York, NY, USA},
author = {Tieleman, Tijmen},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
doi = {10.1145/1390156.1390290},
isbn = {9781605582054},
pages = {1064--1071},
publisher = {Association for Computing Machinery},
series = {ICML '08},
title = {{Training restricted boltzmann machines using approximations to the likelihood gradient}},
year = {2008}
}
@article{Law2015,
abstract = {This paper contributes to the area of inductive logic programming by presenting a new learning framework that allows the learning of weak constraints in Answer Set Programming (ASP). The framework, called Learning from Ordered Answer Sets, generalises our previous work on learning ASP programs without weak constraints, by considering a new notion of examples as ordered pairs of partial answer sets that exemplify which answer sets of a learned hypothesis (together with a given background knowledge) are preferred to others. In this new learning task inductive solutions are searched within a hypothesis space of normal rules, choice rules, and hard and weak constraints. We propose a new algorithm, ILASP2, which is sound and complete with respect to our new learning framework. We investigate its applicability to learning preferences in an interview scheduling problem and also demonstrate that when restricted to the task of learning ASP programs without weak constraints, ILASP2 can be much more efficient than our previously proposed system.},
archivePrefix = {arXiv},
arxivId = {1507.06566},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
doi = {10.1017/S1471068415000198},
eprint = {1507.06566},
issn = {14753081},
journal = {Theory and Practice of Logic Programming},
keywords = {Answer Set Programming,Non-monotonic Inductive Logic Programming,Preference Learning},
number = {4-5},
pages = {511--525},
title = {{Learning weak constraints in answer set programming}},
volume = {15},
year = {2015}
}
@article{zhu2017symbolic,
abstract = {LTLf synthesis is the process of finding a strategy that satisfies a linear temporal specification over finite traces. An existing solution to this problem relies on a reduction to a DFA game. In this paper, we propose a symbolic framework for LTLf synthesis based on this technique, by performing the computation over a representation of the DFA as a boolean formula rather than as an explicit graph. This approach enables strategy generation by utilizing the mechanism of boolean synthesis. We implement this symbolic synthesis method in a tool called Syft, and demonstrate by experiments on scalable benchmarks that the symbolic approach scales better than the explicit one.},
archivePrefix = {arXiv},
arxivId = {1705.08426},
author = {Zhu, Shufang and Tabajara, Lucas M. and Li, Jianwen and Puy, Geguang and Vardi, Moshe Y.},
doi = {10.24963/ijcai.2017/189},
eprint = {1705.08426},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {1362--1369},
title = {{Symbolic LTLf synthesis}},
volume = {0},
year = {2017}
}
@article{Furelos-Blanco2019,
abstract = {In this work we present ISA, a novel approach for learning and exploiting subgoals in reinforcement learning (RL). Our method relies on inducing an automaton whose transitions are subgoals expressed as propositional formulas over a set of observable events. A state-of-the-art inductive logic programming system is used to learn the automaton from observation traces perceived by the RL agent. The reinforcement learning and automaton learning processes are interleaved: a new refined automaton is learned whenever the RL agent generates a trace not recognized by the current automaton. We evaluate ISA in several gridworld problems and show that it performs similarly to a method for which automata are given in advance. We also show that the learned automata can be exploited to speed up convergence through reward shaping and transfer learning across multiple tasks. Finally, we analyze the running time and the number of traces that ISA needs to learn an automata, and the impact that the number of observable events has on the learner's performance.},
archivePrefix = {arXiv},
arxivId = {1911.13152},
author = {Furelos-Blanco, Daniel and Law, Mark and Russo, Alessandra and Broda, Krysia and Jonsson, Anders},
eprint = {1911.13152},
title = {{Induction of Subgoal Automata for Reinforcement Learning}},
url = {http://arxiv.org/abs/1911.13152},
year = {2019}
}
@article{Lamport1994a,
abstract = {We begin by specifying a system that starts with x equal to 0 and keeps incrementing x by 1 forever. In a conventional programming language, this might be written initially x = 0 loop forever x := x + 1end loop The TLA speciication is a formula deened as follows, where the meaning of each conjunct is indicated by the comments. = (x = 0) Initially, x equals 0. {\^{}} 2x 0 = x + 1 ] x Always (2), the value of x in the next state (x 0) equals its value in the current state (x) plus 1. Ignore the subscript x for now. {\^{}} WF x (x 0 = x + 1)Ignore this for now. As speciications get more complicated, we need better methods of writing formulas. We use lists of formulas bulleted with {\^{}} and {\_} to denote conjunc-tions and disjunctions, and we use indentation to eliminate parentheses. The deenition of can then be written as = {\^{}} x = 0 {\^{}} 2x 0 = x + 1 ] x {\^{}} WF x (x 0 = x + 1) What a Formula Means A TLA formula is true or false on a behavior, w h i c h is a sequence of states, where a state is an assignment o f v alues to variables. Formula is true on a behavior in which the i th state assigns the value i ; 1 t o x, f o r i = 1 2 : : : . Systems are reall behaviors are mathematical objects. To decide if a system S satisses formula we m ust have a w ay of representing an execution of S as a behavior (a sequence of states). Given such a represen-tation, we s a y that system S satisses formula (or that S implements the speciication ii (if and only if) is true for every behavior corresponding to a possible execution of S. 1 Another Example Next, we specify a system that starts with x and y both equal to 0 and re-peatedly increments x and y by 1. A step increments either x or y (but not both). The variables are incremented in arbitrary order, but each is incre-mented innnitely often. This system might be represented in a conventional programming language as initially x = 0 , y = 0 cobegin loop forever x := x + 1end loop k loop forever y := y + 1end loop coend},
author = {Lamport, Leslie},
doi = {10.1.1.35.9003},
journal = {SRC Technical Note},
pages = {1994--001},
pmid = {154},
title = {{Introduction to TLA}},
url = {http://research.microsoft.com/en-us/um/people/lamport/tla/all-tla-intro.tex.Z},
volume = {1},
year = {1994}
}
@article{Pawlak1991,
abstract = {This book provides a general discussion of approaches to reasoning about knowledge and its applications to distributed systems, artificial intelligence, and game theory, as well as reasoning about the knowledge of agents who reason about the world.},
author = {Fagin, Ronald and {Y. Halpern}, Joseph and Moses, Yoram and {Y. Vardi}, Moshe},
doi = {10.1007/978-94-011-3534-4_7},
journal = {Rough Sets},
pages = {81--115},
title = {{Reasoning about Knowledge}},
year = {1991}
}
@article{Reif1984,
abstract = {Two-player games of incomplete information have certain portions of positions which are private to each player and cannot be viewed by the opponent. Asymptotically optimal decision algorithms for space bounded games are provided. Various games of incomplete information are presented which are shown to be universal in the sense that they are the hardest of all reasonable games of incomplete information. The problem of determining the outcome of these universal games from a given initial position is shown to be complete in doubly exponential time. "Private alternating Turing machines" are defined to be a new type of alternating Turing machines related to games of incomplete information. The space complexity S(n) of these machines is characterized in terms of the complexity of deterministic Turing machines, with time bounds doubly exponential in S(n). Blindfold games are restricted games in that the second player is not allowed to modify the common position. Asymptotically optimal decision algorithms for space bounded blindfold games are provided. Various blindfold games are also shown to have exponential space complete outcome problems and to be universal for reasonable blindfold games. "Blind alternating Turing machines" are defined to be private alternating Turing machines with restrictions similar to those in blindfold games. The space complexity of these machines is characterized in terms of the complexity of deterministic Turing machines with a single exponential increase in space bounds. {\textcopyright} 1984.},
author = {Reif, John H.},
doi = {10.1016/0022-0000(84)90034-5},
issn = {10902724},
journal = {Journal of Computer and System Sciences},
number = {2},
pages = {274--301},
title = {{The complexity of two-player games of incomplete information}},
volume = {29},
year = {1984}
}
@article{Lee1996,
abstract = {With advanced computer technology, systems are getting larger to fulfill more complicated tasks: however, they are also becoming less reliable. Consequently, testing is an indispensable part of system design and implementation; yet it has proved to be a formidable task for complex systems. This motivates the study of testing jinite state machines to ensure the correct functioning of systems and to discover aspects of their behavior. A Jinite state machine contains a jinite number of states and produces outputs on state transitions after receiving inputs. Finite state machines are widely used to model systems in diverse areas, including sequential circuits, certain types of programs, and, more recently, communication protocols. In a testing problem we have a machine about which we lack some information; we would like to deduce this information by providing a sequence of inputs to the machine and observing the outputs produced. Because of its practical importance and theoretical interest, the problem of testing finite state machines has been studied in different areas and at various times. The earliest published literature on this topic dates back to the 1950's. Activities in the 1960's and early 1970's were motivated mainly by automata theory and sequential circuit testing. The area seemed to have mostly died down until a few years ago when the testing problem was resurrected and is now being studied anew due to its applications to conformance testing of communication protocols. While some old problems which had been open for decades were resolved recently, new concepts and more intriguing problems from new applications to ensure their correct functioning and to discover aspects of their behavior. There are two types of finite state machines: Mealy machines and Moore machines. The theory is very similar for the two types. We consider Mealy machines here because they model finite state systems more properly and are more general than Moore machines. A Mealy machine has a finite number of states and produces outputs on state transitions after receiving inputs. We discuss the following two types of testing problems. In the first type of problems, we have the transition diagram of a finite state machine but we do not know in which state it is. We apply an input sequence to the machine so that from its inputloutput (UO) behavior we can deduce desired information about its state. Specifically, in the state identification problem we wish to identify the initial state of the machine; a test sequence that solves this problem is called a distinguishing sequence. In the state verification problem we wish to verify that the machine is in a specified state; a test sequence that solves this problem is called a unique input/output (UIO) sequence. A different type of problem is conformance testing. Given a specification finite state machine, for which we have its transition diagram, and an imp1ementation, Or box " for which we can only observe its behavior, we want to test whether the implementation conforms to the specification. This is called the conformance testing or fault detection problem and a test sequence that solves this problem is called a checking sequence. Testing hardware and software contains very wide fields with an extensive literature which we cannot hope to cover. Here we will focus on the basic problems of testing finite state machines and present the general principles and methods. We shall not discuss testing combinational circuits which are essentially not finite state systems [50], [75], [l]. We shall not consider functional testing either where we want to verify the equivalence of two known machines or circuits which are not " black boxes " [l], [34], [69]. Numerical software testing is outside the scope of this article where there is an infinite number (in most cases emerge. We review the fundamental problems in testing jinite state ma-chines and techniques for solving these problems, tracing progress in the area from its inception to the present and the state of the art. In addition, we discuss extensions offinite state machines and some other topics related to testing.},
author = {Lee, David and Yannakakis, Mihalis},
doi = {10.1109/5.533956},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {8},
pages = {1090--1123},
title = {{Principles and methods of testing finite state machines - A survey}},
volume = {84},
year = {1996}
}
@book{Aarts,
author = {Aarts, Fides},
isbn = {9789492896728},
title = {{Tomte: Bridging the gap between active learning and real-world systems}}
}
@article{Kucera2005,
abstract = {It is known that LTL formulae without the 'next' operator are invariant under the so-called stutter equivalence of words. In this paper we extend this principle to general LTL formulae with given nesting depths of both 'next' and 'until' operators. This allows us to prove the semantical strictness of three natural hierarchies of LTL formulae, which are parametrized either by the nesting depth of just one of the two operators, or by both of them. Further, we provide an effective characterization of languages definable by LTL formulae with a bounded nesting depth of the 'next' operator. {\textcopyright} Springer-Verlag 2005.},
author = {Ku{\v{c}}era, Anton{\'{i}}n and Strej{\v{c}}ek, Jan},
doi = {10.1007/s00236-005-0164-4},
issn = {00015903},
journal = {Acta Informatica},
number = {7-8},
pages = {415--434},
title = {{The stuttering principle revisited}},
volume = {41},
year = {2005}
}
@article{Markey2006,
author = {Markey, Nicolas and Course, Introductory and Section, Computation},
journal = {Computer},
title = {{Expressiveness of temporal logics}},
year = {2006}
}
@book{B2018,
abstract = {We explore strategies for optimizing selectivity, specificity, and sensitivity in broadband {\{}CARS{\}} by precalculating pulse shapes using an evolutionary algorithm. We show the possibility of selective excitation of a single constituent in a test case of a mixture of five resonant compounds. The obtainable contrast ratio for a test case of {\{}PMMA{\}} in a mixture of five resonant compounds is predicted to be 2000:1, and is related the uniqueness of the complex vibrational response of the compound of interest compared to that of the surrounding molecules. Furthermore we investigate how the effects of homodyne mixing in the focal volume affect the obtainable contrast ratio and how noise affects the optimization. We also show preliminary results of experimental optimization of the {\{}CARS{\}} signal from {\{}PMMA{\}} microspheres, resulting in high contrast imaging, free of non-resonant background signal.},
author = {B, Falk Howar and Steffen, Bernhard},
doi = {10.1007/978-3-319-96562-8},
isbn = {978-3-319-96561-1},
pages = {123--148},
title = {{Machine Learning for Dynamic Software Analysis: Potentials and Limits}},
url = {http://link.springer.com/10.1007/978-3-319-96562-8},
volume = {11026},
year = {2018}
}
@article{Naturwissenschaften2013,
author = {Naturwissenschaften, Doktors Der and Doedt, Markus},
title = {{zur Erlangung des Grades eines Doktors der Naturwissenschaften der Technischen Universit{\"{a}}t Dortmund}},
year = {2013}
}
@article{Angluin1987,
abstract = {The problem of identifying an unknown regular set from examples of its members and nonmembers is addressed. It is assumed that the regular set is presented by a minimally adequate Teacher, which can answer membership queries about the set and can also test a conjecture and indicate whether it is equal to the unknown set and provide a counterexample if not. (A counterexample is a string in the symmetric difference of the correct set and the conjectured set.) A learning algorithm L* is described that correctly learns any regular set from any minimally adequate Teacher in time polynomial in the number of states of the minimum dfa for the set and the maximum length of any counterexample provided by the Teacher. It is shown that in a stochastic setting the ability of the Teacher to test conjectures may be replaced by a random sampling oracle, EX( ). A polynomial-time learning algorithm is shown for a particular problem of context-free language identification. {\textcopyright} 1987.},
author = {Angluin, Dana},
doi = {10.1016/0890-5401(87)90052-6},
issn = {10902651},
journal = {Information and Computation},
number = {2},
pages = {87--106},
title = {{Learning regular sets from queries and counterexamples}},
volume = {75},
year = {1987}
}
@book{Symposium2017,
address = {Cham},
author = {Symposium, Asian and Hutchison, David},
doi = {10.1007/978-3-319-71237-6},
editor = {Chang, Bor-Yuh Evan},
isbn = {978-3-319-71236-9},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Programming Languages and Systems - 15th Asian Symposium, {\{}APLAS{\}} 2017, Suzhou, China, November 27-29, 2017, Proceedings}},
url = {http://link.springer.com/10.1007/978-3-319-71237-6 https://doi.org/10.1007/978-3-319-71237-6},
volume = {10695},
year = {2017}
}
@article{Ahmed1993,
abstract = {This paper extends propositional linear time temporal logic (PTL) to propositional dense time logic (PDTL). While a PTL model is a single sequence of states, a PDTL model, cMled an omega-tree, consists of a nested sequence of states. Two new operators, called within and everywhere are introduced to access nested sequences. Besides its application in describing activities for Artificial Intelligence, PDTL can be used to represent more naturally procedural abstractions in control flow. PDTL is shown to be decidable by a tableau based method, and a complete axiomatization is given. PDTL's omega tree models allow a dense mix of events. By imposing a stability condition on the propositions we get a subset of the omega tree models called ordinal trees which are free of dense mix. This logic called Propositional Ordinal Tree Logic (POTL) is also shown to be decidable in exponential time. Ordinal tree models though based on dense points, represent interval based information which maybe refined to any finite level. Hence POTL is a good bridge between point based and interval based temporal logics. Ordinal trees can be easily embedded as a temporal data structure in a conventional logic programming language and thus provide a framework for temporal logic programming.},
author = {Ahmed, Mohsin and Venkatesh, G.},
doi = {10.1007/3-540-56610-4_91},
isbn = {9783540566106},
issn = {16113349},
journal = {Lecture Notes in Computer Science },
keywords = {Dense time,Ordinal trees,Temporal logic},
pages = {584--598},
title = {{A propositional dense time logic}},
volume = {668 LNCS},
year = {1993}
}
@article{Icarte2018,
abstract = {This paper examines the problem of how to teach multiple tasks to a Reinforcement Learning (RL) agent. To this end, we use Linear Temporal Logic (Lit) as a language for specifying multiple tasks in a manner that supports the composition of learned skills. We also propose a novel algorithm that exploits Lii progression and off- policy RI. to speed up learning without compromising convergence guarantees, and show that our method outperforms the state-of- the-art approach on randomly generated Minecraft-like grids.},
author = {Icarte, Rodrigo Toro and Valenzano, Richard and Kiassen, Toryn Q. and Mcllraith, Sheila A.},
isbn = {9781510868083},
issn = {15582914},
journal = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
keywords = {Deep RL,LTL task specification,Task decomposition},
number = {Aamas},
pages = {452--461},
title = {{Teaching multiple tasks to an RI agent using LTL}},
volume = {1},
year = {2018}
}
@book{Satoh2008,
author = {Satoh, Ken},
booktitle = {Lecture Notes in Computer Science },
isbn = {354078196X},
issn = {03029743},
title = {{Lecture Notes in Artificial Intelligence: Preface}},
volume = {4914 LNAI},
year = {2008}
}
@book{bib:rl-book,
author = {Sutton, Richard S. and Barto, Andrew G.},
edition = {Second},
isbn = {9780262039246},
issn = {2469-9365},
title = {{Reinforcement Learning: An Introduction}},
year = {2018}
}
@article{Alur1995,
author = {Alur, Rajeev and Courcoubetis, Costas and Yannakakis, Mihalis},
doi = {10.1145/225058.225161},
pages = {363--372},
publisher = {Association for Computing Machinery (ACM)},
title = {{Distinguishing tests for nondeterministic and probabilistic machines}},
url = {http://link.springer.com/10.1007/978-3-319-71237-6 https://doi.org/10.1007/978-3-319-71237-6},
year = {1995}
}
@inproceedings{Lamport1983,
author = {Lamport, Leslie},
booktitle = {IFIP Congress Series},
isbn = {0444867295},
pages = {657--668},
publisher = {Elsevier Science Publ Co},
title = {{What good is temporal logic?}},
volume = {9},
year = {1983}
}
@article{Konur2010,
abstract = {This paper surveys main and recent studies on temporal logics in a broad sense by presenting various logic systems, dealing with various time structures, and discussing important features, such as decidability (or undecidability) results, expressiveness and proof systems.},
archivePrefix = {arXiv},
arxivId = {1005.3199},
author = {Konur, Savas},
eprint = {1005.3199},
number = {June},
title = {{A Survey on Temporal Logics}},
url = {http://arxiv.org/abs/1005.3199},
year = {2010}
}
@InCollection{bib:temporal-logics-stanford,
	author       =	{Goranko, Valentin and Rumberg, Antje},
	title        =	{Temporal Logic},
	booktitle    =	{The Stanford Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/sum2020/entries/logic-temporal/}},
	year         =	{2020},
	edition      =	{Summer 2020},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}
@InCollection{bib:pdl-stanford,
	author       =	{Troquard, Nicolas and Balbiani, Philippe},
	title        =	{Propositional Dynamic Logic},
	booktitle    =	{The Stanford Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/spr2019/entries/logic-dynamic/}},
	year         =	{2019},
	edition      =	{Spring 2019},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}
@article{bib:pnueli-ltl,
  title={The temporal logic of programs},
  author={Amir Pnueli},
  journal={18th Annual Symposium on Foundations of Computer Science (sfcs 1977)},
  year={1977},
  pages={46-57}
}
@article{Upadhya2019,
abstract = {The restricted Boltzmann machine (RBM) is a two-layered network of stochastic units with undirected connections between pairs of units in the two layers. The two layers of nodes are called visible and hidden nodes. In an RBM, there are no connections from visible to visible or hidden to hidden nodes. RBMs are used mainly as a generative model. They can be suitably modified to perform classification tasks also. They are among the basic building blocks of other deep learning models such as deep Boltzmann machine and deep belief networks. The aim of this article is to give a tutorial introduction to the restricted Boltzmann machines and to review the evolution of this model.},
author = {Upadhya, Vidyadhar and Sastry, P. S.},
doi = {10.1007/s41745-019-0102-z},
issn = {00194964},
journal = {Journal of the Indian Institute of Science},
number = {2},
pages = {225--236},
title = {{An Overview of Restricted Boltzmann Machines}},
volume = {99},
year = {2019}
}
@inproceedings{10.1007/978-3-319-21690-4_32,
abstract = {In this paper, we present LearnLib, a library for active automata learning. The current, open-source version of LearnLib was completely rewritten from scratch, incorporating the lessons learned from the decade-spanning development process of the previous versions of LearnLib. Like its immediate predecessor, the open-source LearnLib is written in Java to enable a high degree of flexibility and extensibility, while at the same time providing a performance that allows for large-scale applications. Additionally, LearnLib provides facilities for visualizing the progress of learning algorithms in detail, thus complementing its applicability in research and industrial contexts with an educational aspect.},
address = {Cham},
author = {Isberner, Malte and Howar, Falk and Steffen, Bernhard},
booktitle = {Computer Aided Verification},
editor = {Kroening, Daniel and P$\backslash$uas$\backslash$uareanu, Corina S},
isbn = {978-3-319-21690-4},
pages = {487--495},
publisher = {Springer International Publishing},
title = {{The Open-Source LearnLib}},
year = {2015}
}
@article{bib:ga-for-concepts,
abstract = {In this article, we explore the use of genetic algorithms (GAs) as a key element in the design and implementation of robust concept learning systems. We describe and evaluate a GA-based system called GABIL that continually learns and refines concept classification rules from its interaction with the environment. The use of GAs is motivated by recent studies showing the effects of various forms of bias built into different concept learning systems, resulting in systems that perform well on certain concept classes (generally, those well matched to the biases) and poorly on others. By incorporating a GA as the underlying adaptive search mechanism, we are able to construct a concept learning system that has a simple, unified architecture with several important features. First, the system is surprisingly robust even with minimal bias. Second, the system can be easily extended to incorporate traditional forms of bias found in other concept learning systems. Finally, the architecture of the system encourages explicit representation of such biases and, as a result, provides for an important additional feature: the ability to dynamically adjust system bias. The viability of this approach is illustrated by comparing the performance of GABIL with that of four other more traditional concept learners (AQ14, C4.5, ID5R, and IACL) on a variety of target concepts. We conclude with some observations about the merits of this approach and about possible extensions. {\textcopyright} 1993, Kluwer Academic Publishers. All rights reserved.},
author = {de Jong, Kenneth A. and Spears, William M. and Gordon, Diana F.},
doi = {10.1023/A:1022617912649},
issn = {15730565},
journal = {Machine Learning},
keywords = {Concept learning,bias adjustment,genetic algorithms},
number = {2},
pages = {161--188},
title = {Using Genetic Algorithms for Concept Learning},
volume = {13},
year = {1993}
}
@article{Cuccu2019,
abstract = {Deep reinforcement learning, applied to vision-based problems like Atari games, maps pixels directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it By separating the image processing from decision-making, one could better understand the complexity of each task, as well as potentially find smaller policy representations that are easier for humans to understand and may generalize better To this end, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning State representations are generated by an encoder based on two novel algorithms: Increasing Dictionary Vector Quantization makes the encoder capable of growing its dictionary size over time, to address new observations as they appear in an open-ended online-learning context; Direct Residuals Sparse Coding encodes observations by disregarding reconstruction error minimization, and aiming instead for highest information inclusion The encoder autonomously selects observations online to train on, in order to maximize code sparsity As the dictionary size increases, the encoder produces increasingly larger inputs for the neural network: This is addressed by a variation of the Exponential Natural Evolution Strategies algorithm which adapts its probability distribution dimensionality along the run We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on the game's controls) These are still capable of achieving results comparable-and occasionally superior-to state-of-the-art techniques which use two orders of magnitude more neurons.},
archivePrefix = {arXiv},
arxivId = {1806.01363},
author = {Cuccu, Giuseppe and Togelius, Julian and Cudre-Mauroux, Philippe},
eprint = {1806.01363},
isbn = {9781510892002},
issn = {15582914},
journal = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
keywords = {Evolutionary algorithms,Game playing,Learning agent capabilities,Neuroevolution},
pages = {998--1006},
title = {{Playing atari with six neurons}},
volume = {2},
year = {2019}
}
@article{Quint2019,
abstract = {In order to satisfy safety conditions, a reinforcement learned (RL) agent maybe constrained from acting freely, e.g., to prevent trajectories that might cause unwanted behavior or physical damage in a robot. We propose a general framework for augmenting a Markov decision process (MDP) with constraints that are described in formal languages over sequences of MDP states and agent actions. Constraint enforcement is implemented by filtering the allowed action set or by applying potential-based reward shaping to implement hard and soft constraint enforcement, respectively. We instantiate this framework using deterministic finite automata to encode constraints and propose methods of augmenting MDP observations with the state of the constraint automaton for learning. We empirically evaluate these methods with a variety of constraints by training Deep Q-Networks in Atari games as well as Proximal Policy Optimization in MuJoCo environments. We experimentally find that our approaches are effective in significantly reducing or eliminating constraint violations with either minimal negative or, depending on the constraint, a clear positive impact on final performance.},
archivePrefix = {arXiv},
arxivId = {1910.01074},
author = {Quint, Eleanor and Xu, Dong and Dogan, Haluk and Hakguder, Zeynep and Scott, Stephen and Dwyer, Matthew},
eprint = {1910.01074},
number = {NeurIPS},
title = {{Formal Language Constraints for Markov Decision Processes}},
url = {http://arxiv.org/abs/1910.01074},
year = {2019}
}
@article{Andreas2017,
abstract = {We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them-specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor-critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.},
archivePrefix = {arXiv},
arxivId = {1611.01796},
author = {Andreas, Jacob and Klein, Dan and Levine, Sergey},
eprint = {1611.01796},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {229--239},
title = {{Modular multitask reinforcement learning with policy sketches}},
volume = {1},
year = {2017}
}
@article{Yang2019,
abstract = {Inferring behavioral models (e.g., state machines) of software systems is an important element of re-engineering activities. Model inference techniques can be categorized as active or passive learning, constructing models by (dynamically) interacting with systems or (statically) analyzing traces, respectively. Application of those techniques in the industry is, however, hindered by the trade-off between learning time and completeness achieved (active learning) or by incomplete input logs (passive learning). We investigate the learning time/completeness achieved trade-off of active learning with a pilot study at ASML, provider of lithography systems for the semiconductor industry. To resolve the trade-off we advocate extending active learning with execution logs and passive learning results.We apply the extended approach to eighteen components used in ASML TWINSCAN lithography machines. Compared to traditional active learning, our approach significantly reduces the active learning time. Moreover, it is capable of learning the behavior missed by the traditional active learning approach.},
author = {Yang, Nan and Aslam, Kousar and Schiffelers, Ramon and Lensink, Leonard and Hendriks, Dennis and Cleophas, Loek and Serebrenik, Alexander},
doi = {10.1109/SANER.2019.8668007},
isbn = {9781728105918},
journal = {SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
keywords = {active learning,equivalence oracle,model inference,passive learning,reverse engineering,runtime logs},
pages = {253--263},
publisher = {IEEE},
title = {{Improving Model Inference in Industry by Combining Active and Passive Learning}},
year = {2019}
}
@article{Rabinovich1998,
abstract = {The Temporal Logic of Action introduced by Lamport [4] for specifying the behavior of concurrent systems is compared with monadic second order logic which is accepted as an universal formalism for specifying temporal behaviors. The consequences of Lamport's decision to combine in the existential quantifier of TLA, both the standard existential quantifier and the non-logical closure under stuttering are investigated. A continuous time interpretation is provided for TLA and it is argued that this interpretation is more appropriate than the standard discrete time interpretation. Also, some decidability problems are investigated.},
author = {Rabinovich, A.},
doi = {10.1016/S0304-3975(97)00075-3},
issn = {03043975},
journal = {Theoretical Computer Science},
number = {1-2},
pages = {197--214},
title = {{On translations of temporal logic of actions into monadic second-order logic}},
volume = {193},
year = {1998}
}
@article{Tang2020,
abstract = {Inattentional blindness is the psychological phenomenon that causes one to miss things in plain sight. It is a consequence of the selective attention in perception that lets us remain focused on important parts of our world without distraction from irrelevant details. Motivated by selective attention, we study the properties of artificial agents that perceive the world through the lens of a self-attention bottleneck. By constraining access to only a small fraction of the visual input, we show that their policies are directly interpretable in pixel space. We find neuroevolution ideal for training self-attention architectures for vision-based reinforcement learning (RL) tasks, allowing us to incorporate modules that can include discrete, non-differentiable operations which are useful for our agent. We argue that self-attention has similar properties as indirect encoding, in the sense that large implicit weight matrices are generated from a small number of key-query parameters, thus enabling our agent to solve challenging vision based tasks with at least 1000x fewer parameters than existing methods. Since our agent attends to only task critical visual hints, they are able to generalize to environments where task irrelevant elements are modified while conventional methods fail. Videos of our results and source code available at https://attentionagent.github.io/},
archivePrefix = {arXiv},
arxivId = {2003.08165},
author = {Tang, Yujin and Nguyen, Duong and Ha, David},
eprint = {2003.08165},
pages = {1--72},
title = {{Neuroevolution of Self-Interpretable Agents}},
url = {http://arxiv.org/abs/2003.08165},
year = {2020}
}
@article{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
eprint = {1509.02971},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
title = {{Continuous control with deep reinforcement learning}},
year = {2016}
}
@book{Restall1994,
abstract = {Weakening the conditions on the Kripke semantics for propositional intuitionistic logic (J) unearths a family of logics below J. This paper provides a characterization of eleven such logics, using Kripke semantics, proof theory, and algebraic models. Questions about modelling quantification in these logics are also discussed. {\textcopyright} 1994, Duke University Press. All Rights Reserved.},
author = {Restall, Greg},
booktitle = {Notre Dame Journal of Formal Logic},
doi = {10.1305/ndjfl/1040609299},
isbn = {0123456789},
issn = {19390726},
number = {1},
pages = {116--129},
title = {{Subintuitionistic logics}},
volume = {35},
year = {1994}
}
@article{Wang2016,
abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
archivePrefix = {arXiv},
arxivId = {1511.06581},
author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {Van Hasselt}, Hado and Lanctot, Marc and {De Frcitas}, Nando},
eprint = {1511.06581},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
number = {9},
pages = {2939--2947},
title = {{Dueling Network Architectures for Deep Reinforcement Learning}},
volume = {4},
year = {2016}
}
@book{George2002,
abstract = {This book constitutes the refereed proceedings of the 4th International Conference on Formal Engineering methods, ICFEM 2002, held in Shanghai, China, in October 2002.The 43 revised full papers and 16 revised short papers presented together with 5 invited contributions were carefully reviewed and selected from a total of 108 submissions. The papers are organized in topical sections on component engineering and software architecture, method integration, specification techniques and languages, tools and environments, refinement, applications, validation and verification, UML, and semantics.},
author = {George, Chris and Miao, Huaikou},
isbn = {3540000291},
pages = {626},
title = {{Formal methods and software engineering: 4th International ..., Volume 4}},
url = {http://books.google.com/books?id=gerriBADbq4C{\&}pgis=1},
year = {2002}
}
@book{Euzenat2005,
author = {Euzenat, J{\'{e}}r{\^{o}}me and Montanari, Angelo},
doi = {10.1016/s1574-6526(05)80005-7},
isbn = {0444514937},
number = {January 2005},
pages = {59--118},
title = {{Time Granularity}},
year = {2005}
}
@article{Hu2011,
abstract = {We give a formal definition of generalized planning that is independent of any representation formalism. We assume that our generalized plans must work on a set of deterministic environments, which are essentially unrelated to each other. We prove that generalized planning for a finite set of environments is always decidable and EXPSPACE-complete. Our proof is constructive and gives us a sound, complete and complexity-wise optimal technique. We also consider infinite sets of environments, and show that generalized planning for the infinite "one-dimensional problems," known in the literature to be recursively enumerable when restricted to finite-state plans, is EXPSPACE-decidable without sequence functions, and solvable by generalized planning for finite sets.},
author = {Hu, Yuxiao and {De Giacomo}, Giuseppe},
doi = {10.5591/978-1-57735-516-8/IJCAI11-159},
isbn = {9781577355120},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {918--923},
title = {{Generalized planning: Synthesizing plans that work for multiple environments}},
volume = {1},
year = {2011}
}
@article{Hagerer2002,
abstract = {This paper introduces regular extrapolation, a technique that provides descriptions of systems or system aspects a posteriori in a largely automatic way. The descriptions come in the form of models which offer the possibility of mechanically producing system tests, grading test suites and monitoring running systems. Regular extrapolation builds models from observations via techniques from machine learning and finite automata theory. Also expert knowledge about the system enters the model construction in a systematic way. The power of this approach is illustrated in the context of a test environment for telecommunication systems.},
author = {Hagerer, Andreas and Hungar, Hardi and Niese, Oliver and Steffen, Bernhard},
doi = {10.1007/3-540-45923-5_6},
isbn = {3540433538},
issn = {16113349},
journal = {Lecture Notes in Computer Science },
pages = {80--95},
title = {{Model generation by moderated regular extrapolation}},
volume = {2306},
year = {2002}
}
@article{Isberner2014,
abstract = {This paper reviews the development of Register Automaton learning, an enhancement of active automata learning to deal with infinite-state systems. We will revisit the precursor techniques and influences, which in total span over more than a decade. A large share of this development was guided and motivated by the increasingly popular application of grammatical inference techniques in the field of software engineering. We specifically focus on a key problem to achieve practicality in this field: the adequate treatment of data values ranging over infinite domains, a major source of undecidability. Starting with the first case studies, in which data was completely abstracted away, we revisit different steps towards dealing with data explicitly at a model level: we discuss Mealy machines as a model for systems with (data) output, automated alphabet abstraction refinement techniques as a two-dimensional extension of the partition-refinement based approach of active automata learning to also inferring optimal alphabet abstractions, and Register Mealy Machines, which can be regarded as programs restricted to data-independent data processing as it is typical for protocols or interface programs. We are convinced that this development will significantly contribute to paving the road for active automata learning to become a technology of high practical importance.},
author = {Isberner, Malte and Howar, Falk and Steffen, Bernhard},
doi = {10.1007/s10994-013-5419-7},
issn = {15730565},
journal = {Machine Learning},
keywords = {Active automata learning,Alphabet abstraction refinement,Formal methods,Register automata,Software engineering},
number = {1-2},
pages = {65--98},
title = {{Learning register automata: From languages to program structures}},
volume = {96},
year = {2014}
}
@article{Bauer2011,
abstract = {This article studies runtime verification of properties expressed either in lineartime temporal logic (LTL) or timed lineartime temporal logic (TLTL). It classifies runtime verification in identifying its distinguishing features to model checking and testing, respectively. It introduces a three-valued semantics (with truth values true, false, inconclusive) as an adequate interpretation as to whether a partial observation of a running system meets an LTL or TLTL property. For LTL, a conceptually simple monitor generation procedure is given, which is optimal in two respects: First, the size of the generated deterministic monitor is minimal, and, second, the monitor identifies a continuously monitored trace as either satisfying or falsifying a property as early as possible. The feasibility of the developed methodology is demontrated using a collection of real-world temporal logic specifications. Moreover, the presented approach is related to the properties monitorable in general and is compared to existing concepts in the literature. It is shown that the set of monitorable properties does not only encompass the safety and cosafety properties but is strictly larger. For TLTL, the same road map is followed by first defining a three-valued semantics. The corresponding construction of a timed monitor is more involved, yet, as shown, possible. {\textcopyright} 2011 ACM.},
author = {Bauer, Andreas and Leucker, Martin and Schallhart, Christian},
doi = {10.1145/2000799.2000800},
issn = {1049331X},
journal = {ACM Transactions on Software Engineering and Methodology},
keywords = {Assertion checkers,Monitors,Runtime verification},
number = {4},
pages = {1--64},
title = {{Runtime verification for LTL and TLTL}},
volume = {20},
year = {2011}
}
@article{Lamport2002,
author = {Lamport, Leslie},
pages = {364},
title = {{Specifying Systems - TLA}},
url = {http://lamport.org},
year = {2002}
}
@article{Cassel2015,
author = {Cassel, Sofia and Howar, Falk and Jonsson, Bengt and Howar, Falk and Jonsson, Bengt},
journal = {Difts 2015},
title = {{RALib: A LearnLib extension for inferring EFSMs}},
url = {http://www.faculty.ece.vt.edu/chaowang/difts2015/papers/paper{\_}5.pdf{\%}0Ahttps://it.uu.se/katalog/socas172/thesis/difts-final.pdf},
year = {2015}
}
@book{Conference2011,
author = {Conference, International and Hutchison, David},
isbn = {978-3-642-22109-5},
title = {{Computer Aided Verification 2011}},
volume = {23},
year = {2011}
}
@article{Chow1978,
abstract = {We propose a method of testing the correctness of control structures that can be modeled by a finite-state machine. Test results derived from the design are evaluated against the specification. No “executable” prototype is required. The method is based on a result in automata theory and can be applied to software testing. Its error-detecting capability is compared with that of other approaches. Application experience is summarized. {\textcopyright} 1978, IEEE. All rights reserved.},
author = {Chow, Tsun S.},
doi = {10.1109/TSE.1978.231496},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Control structure,finite-state machines,reliability,software testing,test covers,validity},
number = {3},
pages = {178--187},
title = {{Testing Software Design Modeled by Finite-State Machines}},
volume = {SE-4},
year = {1978}
}
@article{Machado2018,
abstract = {The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.},
author = {Machado, Marlos C. and Bellemare, Marc G. and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
doi = {10.1613/jair.5699},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {523--562},
title = {{Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents}},
volume = {61},
year = {2018}
}
@article{Harmelen2007,
author = {Harmelen, Frank and Lifschitz, Vladimir and Porter, Bruce},
isbn = {9780444522115},
journal = {Elsevier Science San Diego, USA},
pages = {1034},
title = {{The Handbook of Knowledge Representation}},
year = {2007}
}
@book{Franceschet2004,
abstract = {The ability of providing and relating temporal representations at different 'grain levels' of the same reality is an important research theme in computer science and a major requirement for many applications, including formal specification and verification, temporal databases, data mining, problem solving, and natural language understanding. In particular, the addition of a granularity dimension to a temporal logic makes it possible to specify in a concise way reactive systems whose behaviour can be naturally modeled with respect to a (possibly infinite) set of differently-grained temporal domains. Suitable extensions of the monadic second-order theory of k successors have been proposed in the literature to capture the notion of time granularity. In this paper, we provide the monadic second-order theories of downward unbounded layered structures, which are infinitely refinable structures consisting of a coarsest domain and an infinite number of finer and finer domains, and of upward unbounded layered structures, which consist of a finest domain and an infinite number of coarser and coarser domains, with expressively complete and elementarily decidable temporal logic counterparts. We obtain such a result in two steps. First, we define a new class of combined automata, called temporalized automata, which can be proved to be the automata-theoretic counterpart of temporalized logics, and show that relevant properties, such as closure under Boolean operations, decidability, and expressive equivalence with respect to temporal logics, transfer from component automata to temporalized ones. Then, we exploit the correspondence between temporalized logics and automata to reduce the task of finding the temporal logic counterparts of the given theories of time granularity to the easier one of finding temporalized automata counterparts of them.},
archivePrefix = {arXiv},
arxivId = {cs/0311022},
author = {Franceschet, Massimo and Montanari, Angelo},
booktitle = {Theory and Practice of Logic Programming},
doi = {10.1017/S147106840400208X},
eprint = {0311022},
isbn = {1471068404},
issn = {14710684},
keywords = {Automata theory,Temporal logic,Time granularity},
number = {5-6},
pages = {621--658},
primaryClass = {cs},
title = {{Temporalized logics and automata for time granularity}},
volume = {4},
year = {2004}
}
@article{bib:temp-logic-automata,
abstract = {The automata-theoretic approach to linear temporal logic uses the theory of automata as a unifying paradigm for program specification, verification, and synthesis. Both programs and specifications are in essence descriptions of computations. These computations can be viewed as words over some alphabet. Thus, programs and specifications can be viewed as descriptions of languages over some alphabet. The automata-theoretic perspective considers the relationships between programs and their specifications as relationships between languages. By translating programs and specifications to automata, questions about programs and their specifications can be reduced to questions about automata. More specifically, questions such as satisfiability of specifications and correctness of programs with respect to their specifications can be reduced to questions such as nonemptiness and containment of automata. Unlike classical automata theory, which focused on automata on finite words, the applications to program specification, verification, and synthesis, use automata on infinite words, since the computations in which we are interested are typically infinite. This paper provides an introduction to the theory of automata on infinite words and demonstrates its applications to program specification, verification, and synthesis.},
annote = {This document gives a good background on complexity and translations. However, I've read up to section 4 (excluded) because I'll use LTL or LDL over finite traces.},
author = {Vardi, Moshe Y.},
isbn = {3540609156},
issn = {16113349},
journal = {Lecture Notes in Computer Science },
pages = {238--266},
title = {{An automata-theoretic approach to linear temporal logic}},
volume = {1043},
year = {1996}
}
@book{Ben-Ari2012,
address = {London},
author = {Ben-Ari, Mordechai},
doi = {10.1007/978-1-4471-4129-7},
isbn = {978-1-4471-4128-0},
publisher = {Springer London},
title = {{Mathematical Logic for Computer Science}},
url = {http://link.springer.com/10.1007/978-1-4471-4129-7},
year = {2012}
}
@article{Erhan2009,
abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work},
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
journal = {Bernoulli},
number = {1341},
pages = {1--13},
title = {{Visualizing higher-layer features of a deep network}},
url = {http://igva2012.wikispaces.asu.edu/file/view/Erhan+2009+Visualizing+higher+layer+features+of+a+deep+network.pdf},
year = {2009}
}
@article{abel2019simple_rl,
author = {Abel, David},
journal = {ICLR Workshop on Reproducibility in Machine Learning},
pages = {1--11},
title = {{simple{\_}rl: Reproducible Reinforcement Learning in Python}},
year = {2019}
}
@misc{Giacomo2019,
author = {Giacomo, Giuseppe De and Iocchi, Luca and Favorito, Marco and Patrizi, Fabio},
number = {Icaps},
pages = {128--136},
title = {{Slides: Foundations for Restraining Bolts : Reinforcement Learning with LTLf / LDLf Restraining Specifications}},
year = {2019}
}
@article{Cassel2016,
abstract = {We present a black-box active learning algorithm for inferring extended finite state machines (EFSM)s by dynamic black-box analysis. EFSMs can be used to model both data flow and control behavior of software and hardware components. Different dialects of EFSMs are widely used in tools for model-based software development, verification, and testing. Our algorithm infers a class of EFSMs called register automata. Register automata have a finite control structure, extended with variables (registers), assignments, and guards. Our algorithm is parameterized on a particular theory, i.e., a set of operations and tests on the data domain that can be used in guards. Key to our learning technique is a novel learning model based on so-called tree queries. The learning algorithm uses tree queries to infer symbolic data constraints on parameters, e.g., sequence numbers, time stamps, identifiers, or even simple arithmetic. We describe sufficient conditions for the properties that the symbolic constraints provided by a tree query in general must have to be usable in our learning model. We also show that, under these conditions, our framework induces a generalization of the classical Nerode equivalence and canonical automata construction to the symbolic setting. We have evaluated our algorithm in a black-box scenario, where tree queries are realized through (black-box) testing. Our case studies include connection establishment in TCP and a priority queue from the Java Class Library.},
author = {Cassel, Sofia and Howar, Falk and Jonsson, Bengt and Steffen, Bernhard},
doi = {10.1007/s00165-016-0355-5},
issn = {1433299X},
journal = {Formal Aspects of Computing},
number = {2},
pages = {233--263},
title = {{Active learning for extended finite state machines}},
volume = {28},
year = {2016}
}
@inproceedings{Berg2005,
abstract = {Conformance testing for finite state machines and regular inference both aim at identifying the model structure underlying a black box system on the basis of a limited set of observations. Whereas the former technique checks for equivalence with a given conjecture model, the latter techniques addresses the corresponding synthesis problem by means of techniques adopted from automata learning. In this paper we establish a common framework to investigate the similarities of these techniques by showing how results in one area can be transferred to results in the other and to explain the reasons for their differences. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Berg, Therese and Grinchtein, Olga and Jonsson, Bengt and Leucker, Martin and Raffelt, Harald and Steffen, Bernhard},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-540-31984-9_14},
issn = {03029743},
pages = {175--189},
title = {{On the correspondence between conformance testing and regular inference}},
volume = {3442},
year = {2005}
}
@article{Steffen2011,
abstract = {In this chapter we give an introduction to active learning of Mealy machines, an automata model particularly suited for modeling the behavior of realistic reactive systems. Active learning is characterized by its alternation of an exploration phase and a testing phase. During exploration phases so-called membership queries are used to construct hypothesis models of a system under learning. In testing phases so-called equivalence queries are used to compare respective hypothesis models to the actual system. These two phases are iterated until a valid model of the target system is produced. We will step-wisely elaborate on this simple algorithmic pattern, its underlying correctness arguments, its limitations, and, in particular, ways to overcome apparent hurdles for practical application. This should provide students and outsiders of the field with an intuitive account of the high potential of this challenging research area in particular concerning the control and validation of evolving reactive systems. {\textcopyright} 2011 Springer-Verlag.},
author = {Steffen, Bernhard and Howar, Falk and Merten, Maik},
doi = {10.1007/978-3-642-21455-4_8},
isbn = {9783642214547},
issn = {03029743},
journal = {Lecture Notes in Computer Science },
number = {Ist 231167},
pages = {256--296},
title = {{Introduction to active automata learning from a practical perspective}},
volume = {6659 LNCS},
year = {2011}
}
@article{asai2019stable,
abstract = {While classical planning has been an active branch of AI, its applicability is limited to the tasks precisely modeled by humans. Fully automated high-level agents should be instead able to find a symbolic representation of an unknown environment without supervision, otherwise it exhibits the knowledge acquisition bottleneck. Meanwhile, Latplan (Asai and Fukunaga 2018) partially resolves the bottleneck with a neural network called State AutoEncoder (SAE). SAE obtains the propositional representation of the image-based puzzle domains with unsupervised learning, generates a state space and performs classical planning. In this paper, we identify the problematic, stochastic behavior of the SAE-produced propositions as a new sub-problem of symbol grounding problem, the symbol stability problem. Informally, symbols are stable when their referents (e.g. propositional values) do not change against small perturbation of the observation, and unstable symbols are harmful for symbolic reasoning. We analyze the problem in Latplan both formally and empirically, and propose "Zero-Suppressed SAE", an enhancement that stabilizes the propositions using the idea of closed-world assumption as a prior for NN optimization. We show that it finds the more stable propositions and the more compact representations, resulting in an improved success rate of Latplan. It is robust against various hyperparameters and eases the tuning effort, and also provides a weight pruning capability as a side effect.},
archivePrefix = {arXiv},
arxivId = {1903.11277},
author = {Asai, Masataro and Kajino, Hiroshi},
eprint = {1903.11277},
isbn = {9781577358077},
issn = {23340843},
journal = {Proceedings International Conference on Automated Planning and Scheduling, ICAPS},
pages = {592--600},
title = {{Towards stable symbol grounding with zero-suppressed State AutoEncoder}},
year = {2019}
}
@inproceedings{10.1007/3-540-48683-6_22,
abstract = {Temporal logic and {\$}\omega{\$}-automata are two ofthe common frameworks for specifying properties of reactive systems in modern verification tools. In this paper we unify these two frameworks in the linear time setting for the specification of stutter-invariant properties, which are used in the context ofpartial-order verification. We will observe a simple variant oflinear time propositional temporal logic (LTL) for expressing exactly the stutter-invariant {\$}\omega{\$}-regular languages. The complexity of, and algorithms for, all the relevant decision procedures for this logic remain essentially the same as with ordinary LTL. In particular, satisfiability remains PSPACE-complete and temporal formulas can be converted to at most exponential sized {\$}\omega{\$}-automata. More importantly, we show that the improved practical algorithms for conversion ofL TL formulas to automata, used in model-checking tools such as SPIN, which typically produce much smaller than worst-case output, can be modified to incorporate this extension to LTL with the same benefits. In this way, the specification mechanism in temporal logic-based tools that employ partial-order reduction can be extended to incorporate all stutter-invariant {\$}\omega{\$}-regular properties.},
address = {Berlin, Heidelberg},
author = {Etessami, Kousha},
booktitle = {Computer Aided Verification},
editor = {Halbwachs, Nicolas and Peled, Doron},
isbn = {978-3-540-48683-1},
pages = {236--248},
publisher = {Springer Berlin Heidelberg},
title = {{Stutter-Invariant Languages, $\omega$-Automata, and Temporal Logic}},
year = {1999}
}
@article{Aichernig2019,
abstract = {System verification is often hindered by the absence of formal models. Peled et al. proposed black-box checking as a solution to this problem. This technique applies active automata learning to infer models of systems with unknown internal structure. This kind of learning relies on conformance testing to determine whether a learned model actually represents the considered system. Since conformance testing may require the execution of a large number of tests, it is considered the main bottleneck in automata learning. In this paper, we describe a randomised conformance testing approach which we extend with fault-based test selection. To show its effectiveness we apply the approach in learning experiments and compare its performance to a well-established testing technique, the partial W-method. This evaluation demonstrates that our approach significantly reduces the cost of learning. In multiple experiments, we reduce the cost by at least one order of magnitude.},
author = {Aichernig, Bernhard K. and Tappler, Martin},
doi = {10.1007/s10817-018-9486-0},
issn = {15730670},
journal = {Journal of Automated Reasoning},
keywords = {Active automata learning,Conformance testing,FSM-based testing,Minimally adequate teacher framework,Mutation testing},
number = {4},
pages = {1103--1134},
publisher = {Springer Netherlands},
title = {{Efficient Active Automata Learning via Mutation Testing}},
url = {https://doi.org/10.1007/s10817-018-9486-0},
volume = {63},
year = {2019}
}
@article{Barto2003,
abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
author = {Barto, Andrew G. and Mahadevan, Sridhar},
doi = {10.1023/A:1022140919877},
issn = {09246703},
journal = {Discrete Event Dynamic Systems: Theory and Applications},
keywords = {Hierarchy,Markov decision processes,Reinforcement learning,Semi-Markov decision processes,Temporal abstraction},
month = {mar},
title = {{Recent Advances in Hierarchical Reinforcement Learning}},
year = {2003}
}
@article{Lamport1994,
abstract = {The temporal logic of actions 1994 is a logic for specifying and reasoning about concurrent systems. Systems and their properties are represented in the same logic, so the assertion that a system meets its specification and the assertion that one system implements another are both expressed by logical implication. TLA is very simple; its syntax and complete formal semantics are summarized in about a page. Yet, TLA is not just a logician's toy; it is extremely powerful, both in principle and in practice. This report introduces TLA and describes how it is used to specify and verify concurrent algorithms. The use of TLA to specify and reason about open systems will be described elsewhere. {\textcopyright} 1994, ACM. All rights reserved.},
author = {Lamport, Leslie},
doi = {10.1145/177492.177726},
issn = {15584593},
journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
keywords = {concurrent programming,liveness properties,safety properties},
number = {3},
pages = {872--923},
title = {{The Temporal Logic of Actions}},
volume = {16},
year = {1994}
}
@article{bib:atari-deeprl,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
eprint = {1312.5602},
pages = {1--9},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Gu2016,
abstract = {Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high- dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-leaming algorithm, which we call normalized advantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.},
archivePrefix = {arXiv},
arxivId = {1603.00748},
author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Uya and Levine, Sergey},
eprint = {1603.00748},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
pages = {4135--4148},
title = {{Continuous deep q-learning with model-based acceleration}},
volume = {6},
year = {2016}
}
@article{Cipollone,
author = {Cipollone, Roberto},
isbn = {9789090284996},
title = {{Note riguardo Model Learning}}
}
@inproceedings{Schuts2016,
abstract = {Many companies struggle with large amounts of legacy software that is difficult to maintain and to extend. Refactoring legacy code typically requires large efforts and introduces serious risks because often crucial business assets are hidden in legacy components. We investigate the support of formal techniques for the rejuvenation of legacy embedded software, concentrating on control components. Model learning and equivalence checking are used to improve a new implementation of a legacy control component. Model learning is applied to both the old and the new implementation. The resulting models are compared using an equivalence check of a model checker. We report about our experiences with this approach at Philips. By gradually increasing the set of input stimuli, we obtained implementations of a power control service for which the learned behaviour is equivalent.},
author = {Schuts, Mathijs and Hooman, Jozef and Vaandrager, Frits},
booktitle = {Lecture Notes in Computer Science },
doi = {10.1007/978-3-319-33693-0_20},
isbn = {9783319336923},
issn = {16113349},
pages = {1--15},
title = {{Refactoring of legacy software using model learning and equivalence checking: An industrial experience report}},
year = {2016}
}
@inproceedings{Chandra1976,
abstract = {We define alternating Turing Machines which are like nondeterministic Turing Machines, except that existential and universal quantifiers alternate. Alternation links up time and space complexities rather well, in that alternating polynomial time equals deterministic polynomial space, and alternating linear space equals deterministic exponential time. Such considerations lead to a two-person game complete in polynomial time, and other games complete in exponential time. We also find that computability on a parallel processing machine is a rather rugged notion, and present two parallel processing models that are polynomially equivalent in their running times. We also show that while n-state alternating finite automata accept only regular sets that can be accepted by 22n-O(logn) state deterministic automata, alternating pushdown automata accept all languages accepted by Turing machines in deterministic exponential time.},
author = {Chandra, Ashok K. and Stockmeyer, Larry J.},
booktitle = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
doi = {10.1109/SFCS.1976.4567893},
issn = {02725428},
keywords = {and phrases,e r m i,n a t i,n g com,n o n t},
title = {{Alternation}},
year = {1976}
}
@article{Vaandrager2017,
author = {Vaandrager, Frits},
doi = {10.1145/2967606},
issn = {15577317},
journal = {Communications of the ACM},
number = {2},
pages = {86--95},
title = {{Model learning}},
volume = {60},
year = {2017}
}
@article{Berges1995,
author = {Berges, Vincent-pierre and Pryzant, Reid},
pages = {1--8},
title = {{Reinforcement Learning for Atari Breakout}},
year = {1995}
}
@article{Zhanson2020,
author = {Zhanson, Josh},
pages = {2020},
title = {{From 0 to 200 - lessons learned from solving Atari Breakout with Reinforcement Learning First stab : Double Deep Q-Network}},
year = {2020}
}
@phdthesis{MorenoFerrando2016,
abstract = {Engineers at the University of Alberta have developed a framework called Arcade Learning Environment (ALE), which allows computers to play Atari videogames based on the generation of random agents and a reward system. This software provides, at each time step, information of the game's RAM, as well as a matrix of pixels that represents the image of the screen. The main goal of this project is to write a program that applies several image processing techniques in order to extract symbolic features from those images. These techniques, implemented in MATLAB, include finding the image's SIFT keypoints and matching them between consecutive frames, segmenting the frame into connected components and obtaining crucial information about each one of them in order to classify the objects as movable or static, and tracking the objects' movements around the screen, in order to estimate an interpolation of their trajectory over time.},
author = {{Moreno Ferrando}, H{\'{e}}ctor},
title = {{Image processing techniques to extract symbolic features from Atari video games}},
url = {http://hdl.handle.net/10230/30828},
year = {2016}
}
@article{Denny2020,
author = {Denny, Hi},
pages = {3--5},
title = {{DQN solution results peak at {\~{}} 35}},
year = {2020}
}
@article{bib:ga-mutations-review,
abstract = {{\textless}p{\textgreater}Genetic algorithm (GA) is an artificial intelligence search method that uses the process of evolution and natural selection theory and is under the umbrella of evolutionary computing algorithm. It is an efficient tool for solving optimization problems. Integration among (GA) parameters is vital for successful (GA) search. Such parameters include mutation and crossover rates in addition to population that are important issues in (GA). However, each operator of GA has a special and different influence. The impact of these factors is influenced by their probabilities; it is difficult to predefine specific ratios for each parameter, particularly, mutation and crossover operators. This paper reviews various methods for choosing mutation and crossover ratios in GAs. Next, we define new deterministic control approaches for crossover and mutation rates, namely Dynamic Decreasing of high mutation ratio/dynamic increasing of low crossover ratio (DHM/ILC), and Dynamic Increasing of Low Mutation/Dynamic Decreasing of High Crossover (ILM/DHC). The dynamic nature of the proposed methods allows the ratios of both crossover and mutation operators to be changed linearly during the search progress, where (DHM/ILC) starts with 100{\%} ratio for mutations, and 0{\%} for crossovers. Both mutation and crossover ratios start to decrease and increase, respectively. By the end of the search process, the ratios will be 0{\%} for mutations and 100{\%} for crossovers. (ILM/DHC) worked the same but the other way around. The proposed approach was compared with two parameters tuning methods (predefined), namely fifty-fifty crossover/mutation ratios, and the most common approach that uses static ratios such as (0.03) mutation rates and (0.9) crossover rates. The experiments were conducted on ten Traveling Salesman Problems (TSP). The experiments showed the effectiveness of the proposed (DHM/ILC) when dealing with small population size, while the proposed (ILM/DHC) was found to be more effective when using large population size. In fact, both proposed dynamic methods outperformed the predefined methods compared in most cases tested.{\textless}/p{\textgreater}},
author = {Hassanat, Ahmad and Almohammadi, Khalid and Alkafaween, Esra'a and Abunawas, Eman and Hammouri, Awni and Prasath, V. B. Surya},
doi = {10.3390/info10120390},
issn = {2078-2489},
journal = {Information},
keywords = {crossover,genetic algorithms,mutation,parameter selection,ratios},
month = {12},
number = {12},
pages = {390},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Choosing Mutation and Crossover Ratios for Genetic Algorithms—A Review with a New Dynamic Approach}},
url = {https://www.mdpi.com/2078-2489/10/12/390},
volume = {10},
year = {2019}
}
@article{Peled1999,
abstract = {DORON PELED Bell Laboratories 600 Mountain Ave. We propose and formalize this problem of and suggest several algorithms.},
author = {Peled, Doron and Vardi, Moshe Y. and Yannakakis, Mihalis},
doi = {10.1007/978-0-387-35578-8_13},
isbn = {9780387355788},
issn = {1430-189X},
number = {January},
pages = {225--240},
title = {{Black Box Checking}},
year = {1999}
}
@article{Clemens2018,
abstract = {Recurrent neural networks (RNNs) are powerful constructs capable of modeling complex systems, up to and including Turing Machines. However, learning such complex models from finite training sets can be difficult. In this paper we empirically show that RNNs can learn models of computer peripheral devices through input and output state observation. This enables automated development of functional software-only models of hardware devices. Such models are applicable to any number of tasks, including device validation, driver development, code de-obfuscation, and reverse engineering. We show that the same RNN structure successfully models six different devices from simple test circuits up to a 16550 UART serial port, and verify that these models are capable of producing equivalent output to real hardware.},
archivePrefix = {arXiv},
arxivId = {1805.07869},
author = {Clemens, John},
doi = {10.1109/IJCNN.2018.8489466},
eprint = {1805.07869},
isbn = {9781509060146},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {1--8},
publisher = {IEEE},
title = {{Learning Device Models with Recurrent Neural Networks}},
volume = {2018-July},
year = {2018}
}
@article{Hasanbeig2019,
abstract = {We propose a method for efficient training of deep Reinforcement Learning (RL) agents when the reward is highly sparse and non-Markovian, but at the same time admits a high-level yet unknown sequential structure, as seen in a number of video games. This high-level sequential structure can be expressed as a computer program, which our method infers automatically as the RL agent explores the environment. Through this process, a high-level sequential task that occurs only rarely may nonetheless be encoded within the inferred program. A hybrid architecture for deep neural fitted Q-iteration is then employed to fill in low-level details and generate an optimal control policy that follows the structure of the program. Our experiments show that the agent is able to synthesise a complex program to guide the RL exploitation phase, which is otherwise difficult to achieve with state-of-the-art RL techniques.},
archivePrefix = {arXiv},
arxivId = {1911.10244},
author = {Hasanbeig, Mohammadhosein and Jeppu, Natasha Yogananda and Abate, Alessandro and Melham, Tom and Kroening, Daniel},
eprint = {1911.10244},
title = {{DeepSynth: Program Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1911.10244},
year = {2019}
}
@article{bib:atari-deepq-nature,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@incollection{Law2014,
address = {Cham},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
doi = {10.1007/978-3-319-11558-0_22},
editor = {Ferm{\'{e}}, Eduardo and Leite, Jo{\~{a}}o},
isbn = {978-3-319-11557-3},
number = {April 2016},
pages = {311--325},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Inductive Learning of Answer Set Programs}},
url = {http://link.springer.com/10.1007/978-3-319-11558-0{\_}22},
volume = {8761},
year = {2014}
}
@article{bib:double-q,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {{Van Hasselt}, Hado and Guez, Arthur and Silver, David},
eprint = {1509.06461},
isbn = {9781577357605},
journal = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
pages = {2094--2100},
title = {{Deep reinforcement learning with double Q-Learning}},
year = {2016}
}
@thesis{bib:favorito-thesis,
author = {Favorito, Marco},
institution = {La Sapienza Universit{\`{a}} di Roma},
title = {{Reinforcement Learning for LTLf / LDLf Goals : Theory and Implementation}},
year = {2018},
type = {mathesis}
}
@article{bib:rainbow,
abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
archivePrefix = {arXiv},
arxivId = {1710.02298},
author = {Hessel, Matteo and Modayil, Joseph and {Van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
eprint = {1710.02298},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
pages = {3215--3222},
title = {{Rainbow: Combining improvements in deep reinforcement learning}},
year = {2018}
}
@article{Ha2018,
abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
archivePrefix = {arXiv},
arxivId = {1803.10122},
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
doi = {10.5281/zenodo.1207631},
eprint = {1803.10122},
pages = {1--24},
title = {{Can agents learn inside of their own dreams ?}},
url = {http://arxiv.org/abs/1803.10122},
year = {2018}
}
@article{bib:degiacomo-logic-nmrdp,
abstract = {In Markov Decision Processes (MDPs), the reward obtained in a state is Markovian, i.e., depends on the last state and action. This dependency makes it difficult to reward more interesting long-term behaviors, such as always closing a door after it has been opened, or providing coffee only following a request. Extending MDPs to handle non-Markovian reward functions was the subject of two previous lines of work. Both use LTL variants to specify the reward function and then compile the new model back into a Markovian model. Building on recent progress in temporal logics over finite traces, we adopt LDL f for specifying non-Markovian rewards and provide an elegant automata construction for building a Markovian model, which extends that of previous work and offers strong minimality and compositionality guarantees.},
author = {Brafman, Ronen I. and {De Giacomo}, Giuseppe and Patrizi, Fabio},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
pages = {1771--1778},
title = {{LTLf / LDLf non-markovian rewards}},
year = {2018}
}
@article{bib:ltlf-ldlf,
abstract = {In this paper we look into the assumption of interpreting LTL over finite traces. In particular we show that LTLf, i.e., LTL under this assumption, is less expressive than what might appear at first sight, and that at essentially no computational cost one can make a significant increase in expressiveness while maintaining the same intuitiveness of LTLf. Indeed, we propose a logic, LDLf for Linear Dynamic Logic over finite traces, which borrows the syntax from Propositional Dynamic Logic (PDL), but is interpreted over finite traces. Satisfiability, validity and logical implication (as well as model checking) for LDLf are PSPACE-complete as for LTLf (and LTL).},
author = {{De Giacomo}, Giuseppe and Vardi, Moshe Y.},
isbn = {9781577356332},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {854--860},
title = {{Linear temporal logic and Linear Dynamic Logic on finite traces}},
year = {2013}
}
@article{DeGiacomo2015,
abstract = {In this paper, we study synthesis from logical specifications over finite traces expressed in LTLf and its extension LDLf. Specifically, in this form of synthesis, propositions are partitioned in controllable and uncontrollable ones, and the synthesis task consists of setting the controllable propositions over time so that, in spite of how the value of the uncontrollable ones changes, the specification is fulfilled. Conditional planning in presence of declarative and procedural trajectory constraints is a special case of this form of synthesis. We characterize the problem computationally as 2EXPTIME-complete and present a sound and complete synthesis technique based on DFA (reachability) games.},
author = {{De Giacomo}, Giuseppe and Vardi, Moshe Y.},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Technical Papers — Planning},
number = {Ijcai},
pages = {1558--1564},
title = {{Synthesis for LTL and LDL on finite traces}},
volume = {2015-Janua},
year = {2015}
}
@article{bib:pdl,
title = {Propositional dynamic logic of regular programs},
journal = {Journal of Computer and System Sciences},
volume = {18},
number = {2},
pages = {194 - 211},
year = {1979},
issn = {0022-0000},
doi = {https://doi.org/10.1016/0022-0000(79)90046-1},
author = {Michael J. Fischer and Richard E. Ladner}
}
@inproceedings{3513276519,
abstract = {We present specification languages that naturally capture exactly the regular and $\omega$-regular properties that are stutter invariant. Our specification languages are variants of the classical regular expressions and of the core of PSL, a temporal logic, which is widely used in industry and which extends the classical linear-time temporal logic LTL by semi-extended regular expressions. {\textcopyright} 2009 Springer-Verlag Berlin Heidelberg.},
author = {Dax, Christian and Klaedtke, Felix and Leue, Stefan},
booktitle = {First publ. in: Automated technology for verification and analysis: 7th International Symposium, ATVA 2009, Macao, China, October 14 - 16, 2009; proceedings / Zhiming Liu ... (eds.). Heidelberg; Berlin: Springer, 2009, pp. 244-254 (=Lecture Notes in Compu},
doi = {10.1007/978-3-642-04761-9_19},
isbn = {3642047602},
issn = {03029743},
number = {October 2009},
pages = {244--254},
title = {{Specification Languages for Stutter-Invariant Regular Properties}},
volume = {5799},
year = {2009}
}
@book{bib:ml-book-murphy,
  title={Machine Learning: A Probabilistic Perspective},
  author={Murphy, K.P.},
  isbn={9780262018029},
  lccn={2012004558},
  series={Adaptive Computation and Machine Learning series},
  year={2012},
  publisher={MIT Press}
}
@inproceedings{bib:probabilistic-rl,
abstract = {Aiming at a comprehensive and concise tutorial survey, recap of variational inference and reinforcement learning with Probabilistic Graphical Models are given with detailed derivations. Reviews and comparisons on recent advances in deep reinforcement learning are made from various aspects. We offer detailed derivations to a taxonomy of Probabilistic Graphical Model and Variational Inference methods in deep reinforcement learning, which serves as a complementary material on top of the original contributions.},
archivePrefix = {arXiv},
arxivId = {1908.09381},
author = {Sun, Xudong and Bischl, Bernd},
booktitle = {2019 IEEE Symposium Series on Computational Intelligence, SSCI 2019},
eprint = {1908.09381},
keywords = {Deep Reinforcement Learning,Probabilistic Graphical Models,Variational Inference},
number = {December},
pages = {110--119},
title = {{Tutorial and Survey on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement Learning}},
year = {2019}
}
@article{bib:atari-games,
abstract = {In this extended abstract we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by presenting a benchmark set of domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. We conclude with a brief update on the latest ALE developments. All of the software, including the benchmark agents, is publicly available.},
archivePrefix = {arXiv},
arxivId = {1207.4708},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
doi = {10.1613/jair.3912},
eprint = {1207.4708},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {4148--4152},
title = {{The arcade learning environment: An evaluation platform for general agents}},
volume = {2015-January},
year = {2015}
}
@inproceedings{bib:nmrdp-logic-first,
author = {Bacchus, Fahiem and Boutilier, Craig and Grove, Adam},
title = {Rewarding Behaviors},
year = {1996},
isbn = {026251091X},
publisher = {AAAI Press},
booktitle = {Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2},
pages = {1160–1167},
numpages = {8},
location = {Portland, Oregon},
series = {AAAI’96}
}
@online{bib:mz-openai-demonstrations,
title = {Learning Montezuma’s Revenge from a Single Demonstration},
date = {2018},
url = {https://openai.com/blog/learning-montezumas-revenge-from-a-single-demonstration/},
}
@book{bib:languages-book,
author = {Hopcroft, John E. and Motwani, Rajeev and Rotwani and Ullman, Jeffrey D.},
title = {Introduction to Automata Theory, Languages and Computability},
year = {2000},
isbn = {0201441241},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
edition = {2nd}
}
@inproceedings{bib:rb-imitation-l,
  author    = {Giuseppe De Giacomo and
               Marco Favorito and
               Luca Iocchi and
               Fabio Patrizi},
  editor    = {J. Christopher Beck and
               Olivier Buffet and
               J{\"{o}}rg Hoffmann and
               Erez Karpas and
               Shirin Sohrabi},
  title     = {Imitation Learning over Heterogeneous Agents with Restraining Bolts},
  booktitle = {Proceedings of the Thirtieth International Conference on Automated
               Planning and Scheduling, Nancy, France, October 26-30, 2020},
  pages     = {517--521},
  publisher = {{AAAI} Press},
  year      = {2020}
}
@article{bib:nfq,
abstract = {This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Riedmiller, Martin},
doi = {10.1007/11564096_32},
isbn = {3540292438},
issn = {03029743},
journal = {Lecture Notes in Computer Science },
pages = {317--328},
title = {{Neural fitted Q iteration - First experiences with a data efficient neural Reinforcement Learning method}},
volume = {3720 LNAI},
year = {2005}
}
@misc{bib:flloat,
  author = {Favorito, Marco},
  title = {FLLOAT},
  url = {https://github.com/whitemech/flloat},
  version = {0.3.0},
}
@article{bib:dbn-learning,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press}
}
@article{bib:rbm-universality,
abstract = {Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton,Osindero, and Teh (2006) alongwith a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improvedmodeling power,while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs. {\textcopyright} 2008 Massachusetts Institute of Technology.},
author = {{Le Roux}, Nicolas and Bengio, Yoshua},
doi = {10.1162/neco.2008.04-07-510},
issn = {08997667},
journal = {Neural Computation},
number = {6},
pages = {1631--1649},
pmid = {18254699},
title = {{Representational power of restricted boltzmann machines and deep belief networks}},
volume = {20},
year = {2008}
}
