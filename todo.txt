+ No new page on section
+ remove page with single image
+ 3.2.1 popular test bed of Deep RL
+ Chapter 3 name Reinforcement learning
+ Split on 3.3 Chaptr "Non-Markovian Reinforcement Learning"
+ Titolo conclusions: future works implicit
+ Nuovo titolo per fluents "Learning to ground symbols trough RL"
+ "Tool for RL with ... (symbols learnt): atariyees package"
+ POMDP da sutton e de giacomo e riscrivi. non lo posso imparare
	- Alcuni POMDP può essere espresso come una dipendenza regolare dalla storia, e poi transformarlo con un MDP esteso
	- RDP "lo stato va 
	- Riformula capitolo. Togli riferimento ai giochi
+ Structure of the thesis: di' quali sono nuovi e quali originali
+ Solve section name same as chapter
+ Solve references to modified chapters
+ Rileggi e unifica capitolo partialobs
- Togli le definizione e usa i finite transducer

- Def 14 Q_1 è per t, Q_r per R
- Definizione con RDP
- Sii preciso ma attieniti al paper
- Transducer devono avere input e output finito
- Non dire features ma fluenti
- Definizione 15  RDP ma con T e R finite transducer
		"nel paper 3 si definivano T and R con logica, qui useremo transducer"
- Teorema RDP si possono tradurre in MDP
- Corollario POMDP to MDP
- Sposta la dichiarazione 
- Dì che sono regolari
- Rimuovi tutti i riferimenti a LDL specification, solo che LDL può rigenerare i linguaggi regolari

- Ask no cite for transducer, right?
